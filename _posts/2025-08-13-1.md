---
layout: single
title: "로컬 PC에서 LLM 학습 시도"
excerpt: "로컬 PC 에서 PEFT/QLoRA 어댑터를 이용한 학습 테스트"
date: 2025-08-13
last_modified_at: 2025-08-13
categories: [AI]
tags: [LLM, PEFT, QLoRA]
toc: true
---

## 공개된 LLM 모델을 이용한 학습 시도
- DAPT (Domain-Adaptive Pretraining, 도메인 적응 사전학습)
  - 기존에 사전학습(pretraining)을 마친 LLM을 특정 도메인의 대규모 텍스트로 다시 사전학습하는 과정입니다.
  - 목적 : 모델의 어휘·문장 패턴·지식 구조를 해당 도메인으로 전반적으로 맞추기.
  - 학습 범위 : 모든 모델 파라미터를 업데이트 (Full model update).
  - 데이터 규모 : 일반적으로 수백 MB~수십 GB 단위의 대규모 비라벨(unsupervised) 텍스트 필요.
  - 결과 : 모델이 해당 도메인에 특화된 언어 이해와 생성 능력을 보유하게 됨.
- PEFT (Parameter-Efficient Fine-Tuning, 파라미터 효율적 파인튜닝)
  - LLM 전체를 재학습하지 않고 일부 파라미터만 학습하거나, 추가 모듈(LoRA, Prefix-tuning 등)만 붙여서 학습하는 방법.
  - 목적 : 적은 자원·시간·저장 공간으로 특정 태스크나 도메인에 맞춘 모델 튜닝.
  - 학습 범위 : 일부 파라미터 또는 추가된 적은 수의 가중치만 학습.
  - 데이터 규모 : 수 MB~수백 MB의 라벨 데이터 또는 소량의 도메인 데이터로 가능.
  - 결과 : 원본 모델의 범용 성능은 유지하면서도, 필요한 작업에서만 특화된 성능 발휘.

## LLM 학습에 적합한 GPU 체크
- 개인 노트북에서 테스트 할 것이기 때문에 학습을 위한 GPU 사용이 가능한지 확인 한다.
- 윈도우 11 OS 기준. cmd 창에서 nvidia-smi 명령을 실행하면 다음과 같이 출력 된다.
<img width="1262" height="302" alt="image" src="https://github.com/user-attachments/assets/a93e843e-2671-4c01-9146-db7bde498116" />
- 여기서 GPU 모델과 VRAM, CUDA 버전을 확인 할 수 있다.

## 학습 과정(PEFT/QLoRA 어댑터 사용)
  1. 로컬 PC 정보
     - CPU i7-77HQ CPU 2.80GHz
     - RAM : 16GB
     - OS : 윈도우 10(64bit)
     - GPU 정보
       - NVIDIA GeForce GTX 1050
       - VRAM : 4096MiB
       - CUDA Version : 12.9
      
  2. 작업 폴더 생성
     - mkdir c:\peft_test
    
  3. 파이썬 설치
     - 가상환경 생성(Python 3.10 권장)
       - py -3.10 -m venv .venv
     - 가상환경 실행
       - .venv/Scripts/Activate.bat
      
  4. 최신 pip
     - python -m pip install -U pip
    
  5. PyTorch (CUDA 12.1 빌드 설치)
     - pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    
  6. 핵심 패키지
     - pip install "transformers==4.43.*" "peft==0.11.*" "trl==0.9.*" "datasets==2.20.*" "accelerate==0.33.*" "sentencepiece==0.2.*" "protobuf>=5.26" "safetensors>=0.4"
    
  7. QLoRA용 bitsandbytes
     - pip install "bitsandbytes==0.43.3"
    
  8. GPU 인식 확인
     - ```python
       import torch
       print("CUDA:", torch.cuda.is_available()) # 실행결과 : CUDA: True
       if torch.cuda.is_available():
           print("GPU:", torch.cuda.get_device_name(0)) # 실행결과 : GPU: NVIDIA GeForce GTX 1050
       ```
  9. 데이터셋
      - 파일명 : mini_instruct.jsonl
      - ```js
        {"messages":[{"role":"user","content":"RAG가 뭐야?"},{"role":"assistant","content":"RAG는 검색(Retrieval)한 문서를 LLM에 넣어 답변 품질을 높이는 기법입니다."}]}
        {"messages":[{"role":"user","content":"LoRA의 핵심 아이디어는?"},{"role":"assistant","content":"큰 가중치를 고정하고 작은 저랭크 행렬만 학습해 효율을 높입니다."}]}
        {"messages":[{"role":"user","content":"QLoRA는 왜 VRAM을 덜 써?"},{"role":"assistant","content":"기존 가중치를 4bit로 로드해 메모리를 크게 줄이기 때문입니다."}]}
        {"messages":[{"role":"user","content":"임베딩은 무엇을 의미해?"},{"role":"assistant","content":"문장이나 토큰을 고정 길이의 숫자 벡터로 바꾸는 표현 방식입니다."}]}
        {"messages":[{"role":"user","content":"Top-k 샘플링이 뭐야?"},{"role":"assistant","content":"확률이 높은 k개의 후보 중 하나를 랜덤 선택하는 생성 전략입니다."}]}
        {"messages":[{"role":"user","content":"Beam search는?"},{"role":"assistant","content":"여러 후보 문장을 유지하며 점수가 높은 경로를 탐색하는 방법입니다."}]}
        {"messages":[{"role":"user","content":"벡터 DB는 왜 써?"},{"role":"assistant","content":"유사도 검색으로 관련 문서를 빠르게 찾기 위해 사용합니다."}]}
        {"messages":[{"role":"user","content":"프롬프트 길이가 너무 길면?"},{"role":"assistant","content":"컨텍스트 윈도 제한에 걸리니 요약/압축 또는 슬라이싱이 필요합니다."}]}
        {"messages":[{"role":"user","content":"한국어 임베딩 모델의 장점은?"},{"role":"assistant","content":"형태소/어순 등 한국어 특성을 반영해 유사도·검색 품질이 좋아집니다."}]}
        {"messages":[{"role":"user","content":"PEFT를 쓰는 가장 큰 이유?"},{"role":"assistant","content":"적은 자원으로 빠르게 튜닝해 실무 적용 비용을 낮출 수 있기 때문입니다."}]}
        {"messages":[{"role":"user","content":"컨텍스트 주입 시 주의점은?"},{"role":"assistant","content":"출처·신뢰도와 최신성, 프롬프트 포맷 통일을 신경 써야 합니다."}]}
        {"messages":[{"role":"user","content":"데이터 품질은 왜 중요해?"},{"role":"assistant","content":"작은 데이터라도 노이즈가 적으면 실제 성능에 크게 기여합니다."}]}
        {"messages":[{"role":"user","content":"지식 업데이트는 어떻게 해?"},{"role":"assistant","content":"RAG로 최신 문서를 검색하거나, 소량의 DAPT/LoRA를 병행합니다."}]}
        {"messages":[{"role":"user","content":"NF4는 뭐지?"},{"role":"assistant","content":"4bit 정규분포 기반 양자화로 QLoRA에서 자주 쓰이는 타입입니다."}]}
        {"messages":[{"role":"user","content":"Latency를 낮추려면?"},{"role":"assistant","content":"짧은 시퀀스, 작은 배치, 캐시 재사용, 경량 모델을 활용하세요."}]}
        {"messages":[{"role":"user","content":"온톨로지는 왜 필요해?"},{"role":"assistant","content":"도메인 용어/스키마를 정리해 검색과 답변의 일관성을 높입니다."}]}
        {"messages":[{"role":"user","content":"프롬프트 엔지니어링 팁은?"},{"role":"assistant","content":"역할·형식·제약·예시를 명확히 하여 모델의 출력 분산을 줄입니다."}]}
        {"messages":[{"role":"user","content":"LLM 평가를 어떻게 해?"},{"role":"assistant","content":"정답 데이터와 메트릭(정확도, 루브릭, 사후 검증)을 함께 씁니다."}]}
        {"messages":[{"role":"user","content":"장기요양 보고서 자동화에 필요한 건?"},{"role":"assistant","content":"문서 템플릿화, 태그/메타데이터, RAG 파이프라인, 검수 워크플로입니다."}]}
        {"messages":[{"role":"user","content":"데이터 라이선스 주의점?"},{"role":"assistant","content":"상업적 이용 가능 여부와 2차 저작권을 꼭 확인해야 합니다."}]}
        {"messages":[{"role":"user","content":"학습이 느릴 때는?"},{"role":"assistant","content":"시퀀스/스텝 감소, bf16→fp16, 체크포인팅 해제 등으로 조정합니다."}]}
        {"messages":[{"role":"user","content":"메모리 부족 에러 해결?"},{"role":"assistant","content":"시퀀스/배치 축소, gradient_accumulation 활용, 4bit 로드 확인."}]}
        {"messages":[{"role":"user","content":"최소 실행으로 무엇을 확인해?"},{"role":"assistant","content":"파이프라인이 돈다, 손실 감소한다, 추론이 달라진다를 확인합니다."}]}
        ```
   10. 학습 스크립트 (QLoRA + Qwen2-0.5B-Instruct)
       - 어댑터 생성을 학습 스크립트
       - 파일명 : train_peft.py
       - ```python
         # 추가 학습한 어댑터를 생성
         
         import os, argparse
         import torch
         from datasets import load_dataset
         from transformers import (
             AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
             TrainingArguments
         )
         from peft import LoraConfig
         from trl import SFTTrainer
         
         def parse_args():
             p = argparse.ArgumentParser()
             p.add_argument("--model", type=str, default="Qwen/Qwen2-0.5B-Instruct")
             p.add_argument("--data_path", type=str, default="mini_instruct.jsonl")
             p.add_argument("--output_dir", type=str, default="out-qwen2-0p5b-lora")
             p.add_argument("--cutoff_len", type=int, default=256)  # 4GB VRAM 안전 구간
             p.add_argument("--max_steps", type=int, default=100)   # 100 step ~ 10~30분 예상(환경차 큼)
             p.add_argument("--lr", type=float, default=2e-4)
             p.add_argument("--per_device_train_batch_size", type=int, default=1)
             p.add_argument("--gradient_accumulation_steps", type=int, default=8)  # 유효 배치 8
             return p.parse_args()
         
         def main():
             args = parse_args()
             os.environ["TOKENIZERS_PARALLELISM"] = "false"
         
             # 4bit 양자화(QLoRA) 설정
             quant_config = BitsAndBytesConfig(
                 load_in_4bit=True,
                 bnb_4bit_use_double_quant=True,
                 bnb_4bit_quant_type="nf4",
                 bnb_4bit_compute_dtype=torch.float16  # Pascal(6.1)에서는 bf16 대신 fp16 권장
             )
         
             tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
             if tokenizer.pad_token is None:
                 tokenizer.pad_token = tokenizer.eos_token
         
             # 모델 로드 (자동 디바이스 매핑)
             model = AutoModelForCausalLM.from_pretrained(
                 args.model,
                 quantization_config=quant_config,
                 torch_dtype=torch.float16,
                 device_map="auto",
             )
         
             # 데이터셋 로드(JSONL: messages list)
             ds = load_dataset("json", data_files=args.data_path, split="train")
         
             # chat 템플릿으로 프리렌더링(text 생성)
             def to_text(example):
                 messages = example["messages"]
                 text = tokenizer.apply_chat_template(
                     messages,
                     tokenize=False,
                     add_generation_prompt=False
                 )
                 return {"text": text}
         
             ds = ds.map(to_text, remove_columns=ds.column_names)
         
             # LoRA 설정
             peft_cfg = LoraConfig(
                 r=8, lora_alpha=16, lora_dropout=0.05,
                 bias="none", task_type="CAUSAL_LM",
                 target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
             )
         
             # 학습 세팅
             train_args = TrainingArguments(
                 output_dir=args.output_dir,
                 per_device_train_batch_size=args.per_device_train_batch_size,
                 gradient_accumulation_steps=args.gradient_accumulation_steps,
                 learning_rate=args.lr,
                 logging_steps=10,
                 bf16=False,
                 fp16=True,
                 max_steps=args.max_steps,
                 save_steps=args.max_steps,
                 save_total_limit=1,
                 optim="paged_adamw_8bit",   # bitsandbytes 옵티마이저
                 lr_scheduler_type="cosine",
                 warmup_ratio=0.03,
                 report_to="none",
             )
         
             # SFTTrainer 사용 (언어모델링 라벨 자동 생성)
             trainer = SFTTrainer(
                 model=model,
                 tokenizer=tokenizer,
                 peft_config=peft_cfg,
                 train_dataset=ds,
                 dataset_text_field="text",
                 max_seq_length=args.cutoff_len,
                 packing=False,  # 예시 데이터가 짧아도 안전하게 off
                 args=train_args,
             )
         
             trainer.train()
             trainer.save_model(args.output_dir)  # 어댑터+메타 저장
             tokenizer.save_pretrained(args.output_dir)
         
         if __name__ == "__main__":
             main()
         
         ```
       - 실행
         - python train_peft.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8
       - 실행로그
         - <details>
           <summary>보기</summary>
           <pre>
           (.venv) C:\peft_test>python train_peft.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8
           The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
           0it [00:00, ?it/s]
           tokenizer_config.json: 1.29kB [00:00, ?B/s]
           C:\peft_test\.venv\lib\site-packages\huggingface_hub\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\kwon\.cache\huggingface\hub\models--Qwen--Qwen2-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
           To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
             warnings.warn(message)
           vocab.json: 2.78MB [00:00, 11.2MB/s]
           merges.txt: 1.67MB [00:00, 26.8MB/s]
           tokenizer.json: 7.03MB [00:00, 17.4MB/s]
           config.json: 100%|████████████████████████████████████████████████████████████████████| 659/659 [00:00<?, ?B/s]
           Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
           model.safetensors: 100%|████████████████████████████████████████████████████| 988M/988M [01:27<00:00, 11.3MB/s]
           generation_config.json: 100%|█████████████████████████████████████████████████████████| 242/242 [00:00<?, ?B/s]
           Generating train split: 23 examples [00:00, 2274.25 examples/s]
           Map: 100%|█████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 183.72 examples/s]
           C:\peft_test\.venv\lib\site-packages\huggingface_hub\utils\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.
           
           Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
             warnings.warn(message, FutureWarning)
           C:\peft_test\.venv\lib\site-packages\trl\trainer\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
             warnings.warn(
           C:\peft_test\.venv\lib\site-packages\trl\trainer\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
             warnings.warn(
           Map: 100%|█████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 735.84 examples/s]
           C:\peft_test\.venv\lib\site-packages\accelerate\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
             self.scaler = torch.cuda.amp.GradScaler(**kwargs)
           max_steps is given, it will override any value given in num_train_epochs
           {'loss': 2.8073, 'grad_norm': 1.8567570447921753, 'learning_rate': 0.0001981178176898239, 'epoch': 3.48}
           {'loss': 1.5486, 'grad_norm': 1.8134342432022095, 'learning_rate': 0.00018687117365181512, 'epoch': 6.96}
           {'loss': 0.7854, 'grad_norm': 2.1768171787261963, 'learning_rate': 0.00016659152250116812, 'epoch': 10.43}
           {'loss': 0.2762, 'grad_norm': 1.6294870376586914, 'learning_rate': 0.00013938757562492873, 'epoch': 13.91}
           {'loss': 0.1143, 'grad_norm': 0.9605799317359924, 'learning_rate': 0.00011452285712454904, 'epoch': 17.39}
           {'loss': 0.0761, 'grad_norm': 0.6575368046760559, 'learning_rate': 8.228090084207774e-05, 'epoch': 20.87}
           {'loss': 0.0655, 'grad_norm': 0.7509737014770508, 'learning_rate': 5.1881405550919493e-05, 'epoch': 24.35}
           {'loss': 0.0628, 'grad_norm': 0.7090865969657898, 'learning_rate': 2.6485360629279977e-05, 'epoch': 27.83}
           {'loss': 0.0606, 'grad_norm': 0.8157389163970947, 'learning_rate': 8.733488479845997e-06, 'epoch': 31.3}
           {'loss': 0.0602, 'grad_norm': 0.724114179611206, 'learning_rate': 4.7165788333860536e-07, 'epoch': 34.78}
           {'train_runtime': 340.2997, 'train_samples_per_second': 2.351, 'train_steps_per_second': 0.294, 'train_loss': 0.5857136249542236, 'epoch': 34.78}
           100%|████████████████████████████████████████████████████████████████████████| 100/100 [05:40<00:00,  3.40s/it]
           </pre>
           </details>
   12. 추론 스크립트 (학습된 어댑터 사용)
       - 파일명 : infer_pert.py
       - ```python
         # 학습된 어댑터(LoRA/QLoRA)를 사용하여 답변을 생성
         
         import torch
         from transformers import AutoTokenizer, AutoModelForCausalLM
         
         # SFTTrainer가 저장한 폴더를 그대로 로드하면, base + adapter 결합 구성이 자동 처리됩니다.
         ADAPTER_DIR = "out-qwen2-0p5b-lora"
         
         tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
         
         model = AutoModelForCausalLM.from_pretrained(
             ADAPTER_DIR,
             torch_dtype=torch.float16,
             device_map="auto"
         )
         model.eval()
         
         def chat(user_text):
             messages = [
                 {"role":"user","content":user_text}
             ]
             prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
             inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
             with torch.no_grad():
                 out = model.generate(
                     **inputs,
                     max_new_tokens=192,
                     do_sample=True,
                     top_p=0.9,
                     temperature=0.7,
                     repetition_penalty=1.1
                 )
             text = tokenizer.decode(out[0], skip_special_tokens=True)
             # 마지막 assistant 답변만 깔끔히 추출(간단 파서)
             sep = "assistant"
             return text.split(sep)[-1].strip()
         
         if __name__ == "__main__":
             print(chat("RAG와 파인튜닝의 차이를 한 문단으로 설명해줘."))
         ```
       - 실행
         - python infer_peft.py

## 결과
- tran_peft 를 실행하여 학습한 어댑터를 만드는 실제 시간은 대략 6~7분 정도 걸렸다.
  - 실행 로그에는 train_runtime : 340 초
  - 로그에서 Deprecated 된 설정값들이 있다는 내용을 확인 하였다.
- infer_peft 를 실행하여 요청한 내용에 대하여 아래와 같이 출력결과가 나왔다.
  - 요청 : RAG와 파인튜닝의 차이를 한 문단으로 설명해줘.
  - 결과 : 적은 자원으로 빠르게 튜닝해드리는 품질을 높이는 방법입니다.
- 어댑터를 사용하지 않은 원본 모델로 동일한 입력값으로 테스트 해 보았는데 결과는 나왔지만 쓸모는 없어 보였다.

## 인사이트 
- 어댑터를 추가적으로 사용한 것과 원본 모델만 사용한 것을 비교 했을때 차이점은 있었다.(추가 학습이 비교적 효과적이라고 보임)
- 하지만 출력 결과에 대한 만족스러운 답은 아니었기에 별도의 튜닝 과정이 반드시 필요하다 판단됨.

## 개선 방향
- 어댑터 생성에 사용된 설정값 중 Deprecated 된 부분 수정이 필요함.
- 답변의 품질을 높이기 위해 아래와 같이 고민 필요함.
  - 현재 모델을 가지고 추가적인 튜닝 여부
  - 더 많은 파라메터를 가진 상위 모델로 교체 여부
- 어댑터 생성시에 발생한 실행 로그를 가독성 있게 시각화 하는 도구 찾기.
- 입력에 대한 출력값이 정확한지 판단하기 위한 검증 도구 찾기.
- 어댑터 생성 실행로그 분석하여 각 내용들이 의미하는 바가 무엇인지 확인.

## 다운로드
 - [소스코드](/assets/files/peft_test.zip)
      
## PEFT 하위 기법 비교표
| 기법                                | 적용 파라미터 수 (대략)                    | 장점                                                    | 단점                                          | 추천 사용 사례                                 |
| --------------------------------- | --------------------------------- | ----------------------------------------------------- | ------------------------------------------- | ---------------------------------------- |
| **LoRA**<br>(Low-Rank Adaptation) | **1\~10%**                        | - 성능 대비 효율 우수<br>- 여러 태스크용 모듈 교체 용이<br>- 병합(merge) 가능 | - 랭크 값 잘못 설정 시 성능 저하<br>- 구조상 적용 레이어 제한     | - 일반 도메인→특정 도메인 전환<br>- 중형\~대형 모델 도메인 튜닝 |
| **QLoRA**<br>(Quantized LoRA)     | **1\~10% (추가 파라미터)**<br>+ 본체 4bit | - VRAM/RAM 절약 극대화<br>- 저사양 GPU에서도 대형 모델 튜닝 가능         | - 양자화로 인한 미세 성능 손실 가능<br>- 4bit 연산 지원 환경 필요 | - LLaMA/Falcon 등 대형 모델 튜닝<br>- 메모리 제한 환경 |
| **Prefix Tuning**                 | **~~0.1~~2%**                     | - 원본 가중치 불변<br>- 문맥 조정 능력 우수                          | - 프롬프트 길이 제한 영향<br>- 태스크별 prefix 관리 필요      | - 대화형 모델 태스크 조정<br>- 스타일·톤 변경            |
| **Prompt Tuning**                 | **~~0.01~~0.1%**                  | - 초경량·매우 빠름<br>- 여러 태스크 프롬프트 교체 용이                    | - 복잡 태스크 성능 한계<br>- 데이터 부족 시 과적합            | - 다중 도메인 분류·간단 질의응답                      |
| **P-Tuning v2**                   | **~~0.1~~1%**                     | - 모든 Transformer 레이어 활용<br>- Prompt Tuning보다 안정적      | - 구현 복잡<br>- 추가 연산량                         | - 소규모 데이터·다양한 아키텍처 지원                    |
| **Adapter Tuning**                | **1\~5%**                         | - 모듈 교체만으로 태스크 전환<br>- 구조 안정성 높음                      | - 레이턴시(추론 지연) 약간 증가<br>- 모델 크기 소폭 증가        | - 멀티태스크 환경<br>- 안정적 장기 유지보수              |
| **BitFit**                        | **<0.1%**                         | - 구현·학습 매우 단순<br>- 데이터 적게 필요                          | - 성능 향상폭 제한적                                | - 빠른 실험·프로토타입 튜닝                         |
| **Side-Tuning**                   | **작은 보조 네트워크 크기**                 | - 원본 모델 완전 보존<br>- 점진적 확장 가능                          | - 구조 복잡<br>- 추론 시 추가 연산                     | - 원본 유지하며 점진 개선 필요할 때                    |
| **Compacter**                     | **~~0.5~~2%**                     | - Adapter보다 더 압축<br>- 다중 태스크 효율 ↑                     | - 구현 난이도↑                                   | - 리소스 제한 + 멀티태스크                         |
| **IA³**                           | **<1%**                           | - Attention 스케일 계수만 학습<br>- 매우 경량                     | - 성능 향상폭 제한                                 | - 경량 조정·파라미터 예산 극소 환경                    |

## 처리 시간 비교 표
| 구분                                  | DAPT (Full Fine-Tuning) | PEFT (예: LoRA)       |
| ----------------------------------- | ----------------------- | -------------------- |
| **업데이트 파라미터 비율**                    | 100%                    | 0.1\~2%              |
| **GPU 메모리 사용량**                     | 매우 높음 (모델 전체)           | 낮음 (추가 모듈만)          |
| **학습 속도**                           | 느림                      | 빠름                   |
| **7B 모델 + 50GB 데이터 (A100 80GB 기준)** | 6\~12시간                 | 1\~3시간               |
| **저사양 GPU 가능 여부**                   | 대부분 불가                  | 가능                   |
| **도메인 적응 깊이**                       | 매우 높음                   | 중간\~높음 (데이터와 기법에 따라) |

- DAPT: 건물 전체를 리모델링 — 벽, 전기 배선, 수도관까지 다 교체 → 오래 걸리지만 완벽한 적응.
- PEFT: 필요한 방만 리모델링 — 최소한의 변경으로 빠른 개선.

