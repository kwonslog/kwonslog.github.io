---
layout: single
title: "로컬 PC에서 LLM 학습 시도"
excerpt: "로컬 PC 에서 공개된 LLM 모델을 이용한 추가 학습 테스트"
date: 2025-08-13
last_modified_at: 2025-08-13
categories: [AI]
tags: [LLM, DAPT]
toc: true
---

# 공개된 LLM 모델을 이용한 학습 시도
- DAPT (Domain-Adaptive Pretraining, 도메인 적응 사전학습)
  - 기존에 사전학습(pretraining)을 마친 LLM을 특정 도메인의 대규모 텍스트로 다시 사전학습하는 과정입니다.
  - 목적 : 모델의 어휘·문장 패턴·지식 구조를 해당 도메인으로 전반적으로 맞추기.
  - 학습 범위 : 모든 모델 파라미터를 업데이트 (Full model update).
  - 데이터 규모 : 일반적으로 수백 MB~수십 GB 단위의 대규모 비라벨(unsupervised) 텍스트 필요.
  - 결과 : 모델이 해당 도메인에 특화된 언어 이해와 생성 능력을 보유하게 됨.
- PEFT (Parameter-Efficient Fine-Tuning, 파라미터 효율적 파인튜닝)
  - LLM 전체를 재학습하지 않고 일부 파라미터만 학습하거나, 추가 모듈(LoRA, Prefix-tuning 등)만 붙여서 학습하는 방법.
  - 목적 : 적은 자원·시간·저장 공간으로 특정 태스크나 도메인에 맞춘 모델 튜닝.
  - 학습 범위 : 일부 파라미터 또는 추가된 적은 수의 가중치만 학습.
  - 데이터 규모 : 수 MB~수백 MB의 라벨 데이터 또는 소량의 도메인 데이터로 가능.
  - 결과 : 원본 모델의 범용 성능은 유지하면서도, 필요한 작업에서만 특화된 성능 발휘.
- 다음 순서로 LLM 학습을 시도한다.
  1. PEFT
  2. DAPT

# LLM 학습에 적합한 GPU 체크
- 개인 노트북에서 테스트 할 것이기 때문에 학습을 위한 GPU 사용이 가능한지 확인 한다.
- 윈도우 11 OS 기준. cmd 창에서 nvidia-smi 명령을 실행하면 다음과 같이 출력 된다.
<img width="1262" height="302" alt="image" src="https://github.com/user-attachments/assets/a93e843e-2671-4c01-9146-db7bde498116" />
- 여기서 GPU 모델과 VRAM, CUDA 버전을 확인 할 수 있다.

# 체크리스트
- [ ] 추가 학습에 사용할 최소 용량의 데이터 준비하기
- [ ] 추가 학습한 내용에 대한 프롬프트 작성 및 실행하기
- [ ] 프롬프트 결과에 대한 평가 방법 확인하기
- [ ] GPU 없이 CPU 만 사용 했을때 가능한 추가 학습 데이터와 LLM 모델 스펙 확인하기
- [ ] 
