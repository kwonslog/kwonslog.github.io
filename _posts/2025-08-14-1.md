---
published: false
layout: single
title: "LLM í•™ìŠµ ê³¼ì • ë¡œê·¸ ë¶„ì„"
excerpt: "wandb ë¥¼ ì´ìš©í•œ í•™ìŠµ ê³¼ì • ë¡œê·¸ ë¶„ì„"
date: 2025-08-14
last_modified_at: 2025-08-14
categories: [AI]
tags: [LLM, wandb]
toc: true
---

# ì²´í¬ë¦¬ìŠ¤íŠ¸
 - [ ] ìˆ˜ì§‘ ëœ ì§€í‘œë“¤ì— ëŒ€í•œ ì„¤ëª…ì¤‘ì— ì´í•´ ì•ˆë˜ëŠ” ë¶€ë¶„ í™•ì¸í•˜ê¸°.
 - [ ] íŒŒì´ì¬ ìµœì‹  ì½”ë“œë¡œ ê²Œì‹œë¬¼ ìˆ˜ì •í•˜ê¸°.
 - [ ] íŒŒì´ì¬ ìµœì‹  ì½”ë“œ ì‹¤í–‰ í›„ í•™ìŠµ ê³¼ì •ì˜ ì½˜ì†” ë¡œê·¸ë¡œ ê²Œì‹œë¬¼ ìˆ˜ì •í•˜ê¸°.



       

## ì‹œì‘
- [ë¡œì»¬ PCì—ì„œ LLM í•™ìŠµ ì‹œë„]({% post_url 2025-08-13-1 %}) ê²Œì‹œë¬¼ì—ì„œ ê°œì„ ë°©í–¥ì— ëŒ€í•´ ì–¸ê¸‰í–ˆë‹¤.
- ê·¸ ì¤‘ì—ì„œ íŒŒì¸ íŠœë‹ í• ë•Œ ë°œìƒí•˜ëŠ” ë¡œê·¸ë¥¼ W&B ì™€ ì—°ë™í•˜ì—¬ ì‹œê°í™” í•˜ëŠ” ë¶€ë¶„ì— ëŒ€í•œ ê³¼ì •ì„ ì •ë¦¬í•´ ë³¸ë‹¤.

## ì‹œê°í™” ê³¼ì •
1. ì‹œê°í™” ë„êµ¬ ì„ ì •(Weights & Biases(W&B))
   - ë§ì´ ì•Œë ¤ì§„ ë„êµ¬ ì¼ ê²ƒ.
   - ë¶„ì„ ì •ë³´ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì•Œê¸° ì‰¬ìš¸ ê²ƒ.
   - ë¡œê·¸ ì „ì†¡ì„ ìœ„í•œ ì—°ë™ ê³¼ì •ì´ ì‰¬ìš¸ ê²ƒ.
  
2. W&B íšŒì› ê°€ì…(https://wandb.ai/)
   - íšŒì› ê°€ì… í• ë•Œ W&B Models, W&B Weave ë¥¼ ì„ íƒ í•  ìˆ˜ ìˆëŠ”ë° ì „ìë¥¼ ì„ íƒí•˜ì—¬ ì§„í–‰í•¨.
   - ê°€ì… ì™„ë£Œ í›„ Home ì— API Key ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.
   - ì²´í—˜íŒ í˜•íƒœë¡œ ê°€ì…ì´ ë˜ë©° ê¸°ê°„ì´ ëë‚˜ë©´ ë¬´ë£Œ í”Œëœìœ¼ë¡œ ì´ìš© ê°€ëŠ¥.
  
3. ê¸°ì¡´ ì½”ë“œ ìˆ˜ì •
   - train_peft.py íŒŒì¼ì— W&B ì—°ë™ì„ ìœ„í•œ ì •ë³´ë¥¼ ì¶”ê°€ í•œ train_peft2_sftconfig.py íŒŒì¼ ì‘ì„±
   - <details markdown="1">
     <summary>ì½”ë“œ</summary>
     
     ```python
     import os, argparse, math, time
     import torch
     from datasets import load_dataset
     from transformers import (
         AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainerCallback
     )
     from peft import LoraConfig
     from trl import SFTTrainer, SFTConfig
     
     # [ì˜µì…˜] W&B
     import wandb
     
     def parse_args():
         p = argparse.ArgumentParser()
         p.add_argument("--model", type=str, default="Qwen/Qwen2-0.5B-Instruct")
         p.add_argument("--data_path", type=str, default="mini_instruct.jsonl")
         p.add_argument("--output_dir", type=str, default="out-qwen2-0p5b-lora")
         p.add_argument("--cutoff_len", type=int, default=256)
         p.add_argument("--max_steps", type=int, default=100)
         p.add_argument("--lr", type=float, default=2e-4)
         p.add_argument("--per_device_train_batch_size", type=int, default=1)
         p.add_argument("--gradient_accumulation_steps", type=int, default=8)
         p.add_argument("--weight_decay", type=float, default=0.0)
         p.add_argument("--lora_dropout", type=float, default=0.05)
         p.add_argument("--val_ratio", type=float, default=0.15)
         p.add_argument("--eval_steps", type=int, default=20)
         p.add_argument("--save_steps", type=int, default=20)
         p.add_argument("--seed", type=int, default=42)
     
         # W&B
         p.add_argument("--use_wandb", action="store_true")
         p.add_argument("--wandb_project", type=str, default="peft_train_test")
         p.add_argument("--wandb_entity", type=str, default=None)
         return p.parse_args()
     
     def main():
         args = parse_args()
         os.environ["TOKENIZERS_PARALLELISM"] = "false"
         torch.manual_seed(args.seed)
     
         # ------ W&B init (ì˜µì…˜) ------
         if args.use_wandb:
             run_name = f"{args.model.split('/')[-1]}-lora-{int(time.time())}"
             wandb.init(
                 project=args.wandb_project,
                 entity=args.wandb_entity,
                 name=run_name,
                 config={
                     "model": args.model,
                     "data_path": args.data_path,
                     "cutoff_len": args.cutoff_len,
                     "max_steps": args.max_steps,
                     "lr": args.lr,
                     "per_device_train_batch_size": args.per_device_train_batch_size,
                     "gradient_accumulation_steps": args.gradient_accumulation_steps,
                     "method": "QLoRA + SFTTrainer (SFTConfig)",
                 },
                 settings=wandb.Settings(_disable_stats=False)  # GPU/ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
             )
     
         # ------ 4bit ì–‘ìí™”(QLoRA) ------
         quant_config = BitsAndBytesConfig(
             load_in_4bit=True,
             bnb_4bit_use_double_quant=True,
             bnb_4bit_quant_type="nf4",
             bnb_4bit_compute_dtype=torch.float16
         )
     
         tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
     
         model = AutoModelForCausalLM.from_pretrained(
             args.model,
             quantization_config=quant_config,
             torch_dtype=torch.float16,
             device_map="auto",
         )
     
         # ------ ë°ì´í„° ë¡œë“œ & ê²€ì¦ ë¶„ë¦¬ ------
         raw = load_dataset("json", data_files=args.data_path)["train"]
         split = raw.train_test_split(test_size=args.val_ratio, seed=args.seed)
         train_ds, val_ds = split["train"], split["test"]
     
         # chat í…œí”Œë¦¿ í”„ë¦¬ë Œë”ë§(text í•„ë“œ ìƒì„±)
         def to_text(example):
             messages = example["messages"]
             text = tokenizer.apply_chat_template(
                 messages, tokenize=False, add_generation_prompt=False
             )
             return {"text": text}
     
         train_ds = train_ds.map(to_text, remove_columns=train_ds.column_names)
         val_ds   = val_ds.map(to_text,   remove_columns=val_ds.column_names)
     
         # ------ LoRA ì„¤ì • ------
         peft_cfg = LoraConfig(
             r=8, lora_alpha=16, lora_dropout=args.lora_dropout,
             bias="none", task_type="CAUSAL_LM",
             target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
         )
     
         # ------ SFTConfig(TrainingArguments ëŒ€ì²´) ------
         sft_cfg = SFTConfig(
             output_dir=args.output_dir,
             per_device_train_batch_size=args.per_device_train_batch_size,
             gradient_accumulation_steps=args.gradient_accumulation_steps,
             learning_rate=args.lr,
             weight_decay=args.weight_decay,
             logging_steps=10,
             bf16=False,
             fp16=True,
             max_steps=args.max_steps,
             save_steps=args.save_steps,
             save_total_limit=1,
             evaluation_strategy="steps",
             eval_steps=args.eval_steps,
             load_best_model_at_end=True,
             metric_for_best_model="eval_loss",
             greater_is_better=False,
             optim="paged_adamw_8bit",
             lr_scheduler_type="cosine",
             warmup_ratio=0.03,
             max_grad_norm=1.0,
             report_to=["wandb"] if args.use_wandb else "none",
             run_name=f"{args.model.split('/')[-1]}-lora" if args.use_wandb else None,
             seed=args.seed,
     
             # SFT ê³ ìœ  ì˜µì…˜
             dataset_text_field="text",
             max_seq_length=args.cutoff_len,
             packing=False,
             gradient_checkpointing=True,        # VRAM ì ˆì•½(ì†ë„ ì•½ê°„ ì €í•˜)
         )
     
         # ------ Trainer ------
         trainer = SFTTrainer(
             model=model,
             tokenizer=tokenizer,
             peft_config=peft_cfg,
             train_dataset=train_ds,
             eval_dataset=val_ds,
             args=sft_cfg,
         )
     
         # ------ í‰ê°€ ì‹œ perplexity ê³„ì‚°/ë¡œê¹… ì½œë°± ------
         # (eval_lossê°€ ì‚°ì¶œëœ í›„ exp(eval_loss) = perplexityë¥¼ ì¶”ê°€ë¡œ ê¸°ë¡)
         class PerplexityCallback(TrainerCallback):
             def on_evaluate(self, args, state, control, **kwargs):
                 metrics = kwargs.get("metrics", {})
                 if "eval_loss" in metrics and metrics["eval_loss"] is not None:
                     try:
                         ppl = math.exp(metrics["eval_loss"])
                     except OverflowError:
                         ppl = float("inf")
                     # ì—¬ê¸°ì„œ trainer.log(...) í˜¸ì¶œ âŒ
                     # metricsì—ë§Œ ì¶”ê°€í•˜ê³  control ë°˜í™˜ âœ…
                     metrics["perplexity"] = ppl
                 return control
     
         trainer.add_callback(PerplexityCallback())
     
         # ------ í•™ìŠµ & ì €ì¥ ------
         trainer.train()
         trainer.save_model(args.output_dir)
         tokenizer.save_pretrained(args.output_dir)
     
         if args.use_wandb:
             wandb.finish()
     
     if __name__ == "__main__":
         main()
          
     ```
     
     </details>
     
5. W&B Project ìƒì„±
   - W&B ì›¹ ì‚¬ì´íŠ¸ì—ì„œ Projects íƒ­ì—ì„œ ì›í•˜ëŠ” ì´ë¦„ì˜ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±.(ì•„ì£¼ ê°„ë‹¨í•¨)
  
6. W&B ë¡œê·¸ì¸
   - íŒŒì´ì¬ ê°€ìƒí™˜ê²½ì„ ì‹¤í–‰í•˜ê³  íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— W&B ë¡œê·¸ì¸ì„ í•œë‹¤.
     - pip install wandb
     - wandb login
     - APi key ì…ë ¥í•˜ë©´ ë.
     - ì‹¤í–‰ ë¡œê·¸
       <details>
         <summary>ë³´ê¸°</summary>
         <pre>
         wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
         wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models
         wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:
         wandb: No netrc file found, creating one.
         wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\kwon\_netrc
         wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin           
         </pre>
       </details>
7. ì½”ë“œ ì‹¤í–‰
   - python train_peft2_sftconfig3.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --eval_steps 20 --save_steps 20 --use_wandb --wandb_project local-pc-test
   - ë§ˆì§€ë§‰ --wandb_project ê°’ì€ W&Bì—ì„œ ìƒì„±í•œ í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì‚¬ìš©í–ˆë‹¤.
   - ì‹¤í–‰ ë¡œê·¸
     <details>
       <summary>ë³´ê¸°</summary>
       <pre>
       (.venv) C:\peft_test>python train_peft2_sftconfig3.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --eval_steps 20 --save_steps 20 --use_wandb --wandb_project local-pc-test
       wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
       wandb: Tracking run with wandb version 0.21.1
       wandb: Run data is saved locally in C:\peft_test\wandb\run-20250818_104047-14muyjlm
       wandb: Run `wandb offline` to turn off syncing.
       wandb: Syncing run Qwen2-0.5B-Instruct-lora-1755481247
       wandb:  View project at https://wandb.ai/kwonslog-study/local-pc-test
       wandb:  View run at https://wandb.ai/kwonslog-study/local-pc-test/runs/14muyjlm
       C:\peft_test\.venv\lib\site-packages\transformers\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
         warnings.warn(
       Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 71.80 examples/s]
       C:\peft_test\.venv\lib\site-packages\accelerate\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
         self.scaler = torch.cuda.amp.GradScaler(**kwargs)
       max_steps is given, it will override any value given in num_train_epochs
         0%|                                                                                          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
       C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 2.7509, 'grad_norm': 1.7712396383285522, 'learning_rate': 0.0001981178176898239, 'epoch': 4.21}
       {'loss': 1.413, 'grad_norm': 2.205397129058838, 'learning_rate': 0.00018687117365181512, 'epoch': 8.42}
        20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 20/100 [01:31<06:21,  4.77s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
       {'eval_loss': 2.680995464324951, 'eval_runtime': 0.4523, 'eval_samples_per_second': 8.843, 'eval_steps_per_second': 2.211, 'epoch': 8.42}
        20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                                | 20/100 [01:32<06:21,  4.77s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.6029, 'grad_norm': 1.7841202020645142, 'learning_rate': 0.00016659152250116812, 'epoch': 12.63}
       {'loss': 0.1577, 'grad_norm': 1.1527832746505737, 'learning_rate': 0.00013938757562492873, 'epoch': 16.84}
       {'eval_loss': 3.554626226425171, 'eval_runtime': 0.4171, 'eval_samples_per_second': 9.59, 'eval_steps_per_second': 2.397, 'epoch': 16.84}
        40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                                | 40/100 [03:10<04:50,  4.85s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0722, 'grad_norm': 0.8666030764579773, 'learning_rate': 0.00010808804403614043, 'epoch': 21.05}
       {'loss': 0.0632, 'grad_norm': 0.865715503692627, 'learning_rate': 7.594750436337467e-05, 'epoch': 25.26}
       {'eval_loss': 3.5855770111083984, 'eval_runtime': 0.4205, 'eval_samples_per_second': 9.512, 'eval_steps_per_second': 2.378, 'epoch': 25.26}
        60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 60/100 [04:49<03:15,  4.89s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0588, 'grad_norm': 0.6691780686378479, 'learning_rate': 4.630798263510162e-05, 'epoch': 29.47}
       {'loss': 0.0568, 'grad_norm': 0.6350418925285339, 'learning_rate': 2.2251444932035094e-05, 'epoch': 33.68}
       {'eval_loss': 3.633697271347046, 'eval_runtime': 0.4166, 'eval_samples_per_second': 9.602, 'eval_steps_per_second': 2.401, 'epoch': 33.68}
        80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 80/100 [06:24<01:33,  4.67s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0562, 'grad_norm': 0.6472147703170776, 'learning_rate': 6.2793294993656494e-06, 'epoch': 37.89}
       {'loss': 0.0554, 'grad_norm': 0.6560364365577698, 'learning_rate': 5.2443095448506674e-08, 'epoch': 42.11}
       {'eval_loss': 3.6372601985931396, 'eval_runtime': 0.4229, 'eval_samples_per_second': 9.458, 'eval_steps_per_second': 2.364, 'epoch': 42.11}
       {'train_runtime': 492.1056, 'train_samples_per_second': 1.626, 'train_steps_per_second': 0.203, 'train_loss': 0.5287103277444839, 'epoch': 42.11}
       100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [08:12<00:00,  4.92s/it]
       wandb:
       wandb:
       wandb: Run history:
       wandb:               eval/loss â–â–‡â–ˆâ–ˆâ–ˆ
       wandb:            eval/runtime â–ˆâ–â–‚â–â–‚
       wandb: eval/samples_per_second â–â–ˆâ–‡â–ˆâ–‡
       wandb:   eval/steps_per_second â–â–ˆâ–‡â–ˆâ–‡
       wandb:             train/epoch â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–‡â–ˆâ–ˆâ–ˆ
       wandb:       train/global_step â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–†â–†â–†â–‡â–ˆâ–ˆâ–ˆ
       wandb:         train/grad_norm â–†â–ˆâ–†â–ƒâ–‚â–‚â–â–â–â–
       wandb:     train/learning_rate â–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–‚â–â–
       wandb:              train/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–
       wandb:
       wandb: Run summary:
       wandb:                eval/loss 3.63726
       wandb:             eval/runtime 0.4229
       wandb:  eval/samples_per_second 9.458
       wandb:    eval/steps_per_second 2.364
       wandb:               total_flos 99576665223168.0
       wandb:              train/epoch 42.10526
       wandb:        train/global_step 100
       wandb:          train/grad_norm 0.65604
       wandb:      train/learning_rate 0.0
       wandb:               train/loss 0.0554
       wandb:               train_loss 0.52871
       wandb:            train_runtime 492.1056
       wandb: train_samples_per_second 1.626
       wandb:   train_steps_per_second 0.203
       wandb:
       wandb:  View run Qwen2-0.5B-Instruct-lora-1755481247 at: https://wandb.ai/kwonslog-study/local-pc-test/runs/14muyjlm
       wandb:  View project at: https://wandb.ai/kwonslog-study/local-pc-test
       wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
       wandb: Find logs at: .\wandb\run-20250818_104047-14muyjlm\logs
       </pre>
     </details>
     
8. W&B ë¡œê·¸ í™•ì¸
   - View project at: https://wandb.ai/kwonslog-study/local-pc-test
   - ìœ„ì™€ ê°™ì´ í™•ì¸ ê°€ëŠ¥í•œ ì£¼ì†Œê°€ ì¶œë ¥ë˜ë©° í•´ë‹¹ ì£¼ì†Œë¥¼ í†µí•´ ì‹œê°í™” ëœ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.
   - ì˜ˆì‹œ
     <img width="1788" height="821" alt="image" src="https://github.com/user-attachments/assets/0f0ceb55-3faa-48f7-93de-4b5f9e978b0d" />

## ì‹œê°í™” ë°ì´í„° ë¶„ì„
- ë¨¼ì € ê° ì§€í‘œë“¤ì˜ ê°œë…ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì.
- W&B ì˜ train ëŒ€ì‹œë³´ë“œì—ëŠ” ì•„ë˜ ì§€í‘œë“¤ì— ëŒ€í•œ ê·¸ë˜í”„ê°€ ë³´ì¸ë‹¤.
   - loss
   - learning_rate
   - grad_norm
   - global_step
   - epoch
 
### loss (ì†ì‹¤ê°’)
- ëª¨ë¸ì´ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ ì˜ˆì¸¡ì„ í–ˆëŠ”ì§€ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚¸ ê°’.
- 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ ì˜ˆì¸¡ì´ ì •ë‹µì— ê°€ê¹ë‹¤ëŠ” ëœ».
- í•™ìŠµì—ì„œëŠ” ì†ì‹¤ â†’ ì—­ì „íŒŒ â†’ ê°€ì¤‘ì¹˜ ìˆ˜ì •ì˜ í•µì‹¬ ë£¨í”„ë¥¼ ë•ëŠ” ì§€í‘œ.

| ê°’ ë³€í™”               | í•´ì„ ì˜ˆì‹œ                                  |
| ------------------ | -------------------------------------- |
| í¬ë‹¤ (ì˜ˆ: 5.0 ì´ìƒ) | ì˜ˆì¸¡ì´ ì •ë‹µê³¼ ë§ì´ ë‹¤ë¦„ â†’ í•™ìŠµ ì´ˆê¸°ê±°ë‚˜, í•™ìŠµì´ ì˜ ì•ˆ ë˜ëŠ” ìƒí™© |
| ì‘ë‹¤ (ì˜ˆ: 0.5 ì´í•˜) | ì˜ˆì¸¡ì´ ì •ë‹µì— ê°€ê¹Œì›€ â†’ í•™ìŠµì´ ì§„í–‰ë¨, ì„±ëŠ¥ ì¢‹ì•„ì§          |
| ì¤„ë‹¤ê°€ ë‹¤ì‹œ ì»¤ì§      | ê³¼ì í•© ê°€ëŠ¥ì„± â†’ í•™ìŠµ ë°ì´í„°ì—” ì˜ ë§ì§€ë§Œ ì¼ë°˜í™”ê°€ ì•ˆ ë˜ê³  ìˆìŒ   |

  
### learning_rate (í•™ìŠµë¥ )
- í•œ ë²ˆì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ì—ì„œ ì–¼ë§ˆë‚˜ í¬ê²Œ ì›€ì§ì¼ì§€ë¥¼ ì •í•˜ëŠ” ê°’.
- ë„ˆë¬´ í¬ë©´ â†’ ë°œì‚°(í•™ìŠµ ë¶ˆì•ˆì •), ë„ˆë¬´ ì‘ìœ¼ë©´ â†’ í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë ¤ì§.
- ë³´í†µ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•´ í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì¤„ì´ê±°ë‚˜ ì¡°ì ˆ.

| ê°’ ë³€í™”                     | í•´ì„ ì˜ˆì‹œ                          |
| ------------------------ | ------------------------------ |
| í¬ë‹¤ (ì˜ˆ: 1e-2)         | ë¹ ë¥´ê²Œ í•™ìŠµí•˜ì§€ë§Œ ë¶ˆì•ˆì • â†’ lossê°€ ìš”ë™ì¹  ìˆ˜ ìˆìŒ |
| ì ë‹¹ (ì˜ˆ: 1e-3 \~ 1e-4) | ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë¨ (ë§ì´ ì“°ëŠ” ë²”ìœ„)           |
| ì•„ì£¼ ì‘ë‹¤ (ì˜ˆ: 1e-6 ì´í•˜)   | ê±°ì˜ í•™ìŠµì´ ì•ˆ ë¨, lossê°€ ì˜ ì•ˆ ì¤„ì–´ë“¦      |


### grad_norm (ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°)
- í•œ ìŠ¤í…ì—ì„œ ê³„ì‚°ëœ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰(gradient)ì˜ í¬ê¸°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ ê°’.
- í•™ìŠµ ì•ˆì •ì„±ì˜ ì§€í‘œ. ë„ˆë¬´ í¬ë©´ í­ë°œ, ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµ ë©ˆì¶¤.
- max_grad_normìœ¼ë¡œ í´ë¦¬í•‘(ì œí•œ) ê°€ëŠ¥.

| ê°’ ë³€í™”                   | í•´ì„ ì˜ˆì‹œ                                |
| ---------------------- | ------------------------------------ |
| í¬ë‹¤ (ì˜ˆ: 10 ì´ìƒ)      | Gradient í­ë°œ ê°€ëŠ¥ì„± â†’ í•™ìŠµ ë¶ˆì•ˆì •, loss ê¸‰ë“± ê°€ëŠ¥ |
| ì ë‹¹ (ì˜ˆ: 1\~5)       | ì •ìƒ ë²”ìœ„, ì•ˆì •ì ì¸ ì—…ë°ì´íŠ¸                     |
| ì•„ì£¼ ì‘ë‹¤ (ì˜ˆ: 0.01 ì´í•˜) | í•™ìŠµ ì •ì²´, ê°€ì¤‘ì¹˜ê°€ ê±°ì˜ ì•ˆ ë³€í•¨                  |

### global_step (ì „ì²´ ìŠ¤í… ìˆ˜)
- ì§€ê¸ˆê¹Œì§€ í›ˆë ¨ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•œ íšŸìˆ˜.
- í•™ìŠµ ì§„í–‰ ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ìˆœ ì¹´ìš´í„°.
- ì˜ˆ: batch size=1, ë°ì´í„° 100ê°œ â†’ 100ìŠ¤í…ì´ í•œ ì—í­.

| ê°’ ë³€í™”        | í•´ì„ ì˜ˆì‹œ                      |
| ----------- | -------------------------- |
| ì‘ë‹¤ (ì´ˆë°˜) | í•™ìŠµ ë§‰ ì‹œì‘, lossê°€ í¬ê²Œ ë³€í•  ìˆ˜ ìˆìŒ  |
| í¬ë‹¤ (í›„ë°˜) | í•™ìŠµ í›„ë°˜ë¶€, lossê°€ ì•ˆì •ë˜ê±°ë‚˜ ê³¼ì í•© ê°€ëŠ¥ |

### epoch (ì—í­)
- í•™ìŠµ ë°ì´í„° ì „ì²´ë¥¼ í•œ ë²ˆ ë‹¤ ë³¸ ì£¼ê¸°.
- 1ì—í­ = ë°ì´í„° ì „ë¶€ í•™ìŠµ í•œ ë²ˆ ì™„ë£Œ.
- ì—¬ëŸ¬ ì—í­ ë°˜ë³µí•˜ë©´ì„œ ì ì  ì„±ëŠ¥ í–¥ìƒ.

| ê°’ ë³€í™”              | í•´ì„ ì˜ˆì‹œ                           |
| ----------------- | ------------------------------- |
| ì‘ë‹¤ (1\~2)     | í•™ìŠµ ì´ˆê¸°, ì„±ëŠ¥ í–¥ìƒ í­ì´ í¼               |
| ì¤‘ê°„ (3\~10)    | ì•ˆì •í™” ë‹¨ê³„, loss ê°ì†Œí­ì´ ì¤„ì–´ë“¦           |
| ë„ˆë¬´ í¬ë‹¤ (20 ì´ìƒ) | ê³¼ì í•© ìœ„í—˜ â†’ eval ì„±ëŠ¥ì´ ë–¨ì–´ì§€ê¸° ì‹œì‘í•  ìˆ˜ ìˆìŒ |

### ì¢…í•© í•´ì„ ì˜ˆì‹œ
- ì •ìƒ í•™ìŠµ íŒ¨í„´
  - train loss: ì ì  í•˜ë½
  - learning_rate: ìŠ¤ì¼€ì¤„ì— ë”°ë¼ ì„œì„œíˆ ê°ì†Œ
  - grad_norm: ì•ˆì • ë²”ìœ„(1\~5) ìœ ì§€
  - global_step / epoch: ê¾¸ì¤€íˆ ì¦ê°€
    
- ë¬¸ì œ íŒ¨í„´ ì˜ˆì‹œ
  - lossê°€ ì¤„ì§€ ì•ŠìŒ â†’ learning_rate ë„ˆë¬´ ë‚®ìŒ or ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ
  - grad_normì´ ê³„ì† ë†’ìŒ â†’ learning_rate ê³¼ë‹¤ or ëª¨ë¸ ë¶ˆì•ˆì •
  - train lossëŠ” ë‚®ì§€ë§Œ eval loss ìƒìŠ¹ â†’ ê³¼ì í•©

### W&B ì˜ ì§€í‘œ ë¶„ì„ 
- ì—°ë™í•œ W&Bì˜ ì§€í‘œ(ê·¸ë˜í”„)ë¥¼ ë³´ê³  í•™ìŠµ ìƒí™©ì´ ì–´ë–¤ì§€ ë¶„ì„í•´ ë³¸ë‹¤.
  
1. ê³¼ì í•©
<img width="1284" height="433" alt="image" src="https://github.com/user-attachments/assets/008f0b12-ee9d-4776-803a-ced682f3bd7b" />

- train/loss
  - í•™ìŠµ ìŠ¤í…ì´ ì§„í–‰ë ìˆ˜ë¡ ë¹ ë¥´ê²Œ ê°ì†Œ â†’ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ì ì  ì˜ ë§ì¶”ê³  ìˆìŒ.
  - 40ìŠ¤í… ì´í›„ ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ â†’ í•™ìŠµ ë°ì´í„°ëŠ” ì™„ë²½í•˜ê²Œ ì™¸ì›Œê°€ëŠ” ìƒíƒœ.
- eval/loss
  - ì´ˆê¸°ì—” ë‚®ì•˜ë‹¤ê°€, ìŠ¤í…ì´ ì§„í–‰ë ìˆ˜ë¡ ì˜¤íˆë ¤ ìƒìŠ¹ â†’ ìƒˆë¡œìš´ ë°ì´í„°(ê²€ì¦ì…‹)ì—ì„œëŠ” ì„±ëŠ¥ì´ ì ì  ë‚˜ë¹ ì§.
  - 20ìŠ¤í… ì´í›„ë¶€í„°ëŠ” ì§€ì†ì ìœ¼ë¡œ ì¦ê°€ â†’ í•™ìŠµ ë°ì´í„°ì—ë§Œ íŠ¹í™”ë˜ê³  ì¼ë°˜í™” ëŠ¥ë ¥ ì €í•˜.

| ì§€í‘œ ë³€í™”                   | ì˜ë¯¸                |
| ----------------------- | ----------------- |
| **train loss â†“â†“**       | í•™ìŠµ ë°ì´í„° ì˜ˆì¸¡ ì •í™•ë„ ê¸‰ìƒìŠ¹ |
| **eval loss â†‘â†‘**        | ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ ì„±ëŠ¥ í•˜ë½   |
| train / eval ê³¡ì„ ì˜ ë°©í–¥ì´ ë°˜ëŒ€ | **ì „í˜•ì ì¸ ê³¼ì í•©** íŒ¨í„´   |

     
