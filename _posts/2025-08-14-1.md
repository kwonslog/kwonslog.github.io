---
published: false
layout: single
title: "LLM 학습 과정 로그 분석"
excerpt: "wandb 를 이용한 학습 과정 로그 분석"
date: 2025-08-14
last_modified_at: 2025-08-14
categories: [AI]
tags: [LLM, wandb]
toc: true
---

## 시작
- [로컬 PC에서 LLM 학습 시도]({% post_url 2025-08-13-1 %}) 게시물에서 개선방향에 대해 언급했다.
- 그 중에서 학습 과정 중에 console 에 출력되는 로그를 wandb 와 연동하여 시각화 하는 부분에 대한 학습 과정을 정리해 본다.

## 과정
1. 시각화 도구 선정(Weights & Biases(W&B))
   - 많이 알려진 도구 일 것.
   - 분석 정보를 직관적으로 알기 쉬울 것.
   - 로그 전송을 위한 연동 과정이 쉬울 것.
  
2. W&B 회원 가입(https://wandb.ai/)
   - 회원 가입 할때 W&B Models, W&B Weave 를 선택 할 수 있는데 전자를 선택하여 진행함.
   - 가입 완료 후 Home 에 API Key 를 확인 할 수 있다.
   - 체험판 형태로 가입이 되며 기간이 끝나면 무료 플랜으로 이용 가능.
  
4. 기존 코드 수정
   - train_peft.py 파일에 W&B 연동을 위한 정보를 추가 한다.
   - <details markdown="1">
     <summary>코드</summary>
     ```python
     # 추가 학습한 어댑터를 생성 + 생성과정 시각화(W&B) 연동
     
     import os, argparse
     import torch
     from datasets import load_dataset
     from transformers import (
         AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
         TrainingArguments
     )
     from peft import LoraConfig
     from trl import SFTTrainer
     
     # [추가] W&B
     import time
     import wandb
     
     def parse_args():
         p = argparse.ArgumentParser()
         p.add_argument("--model", type=str, default="Qwen/Qwen2-0.5B-Instruct")
         p.add_argument("--data_path", type=str, default="mini_instruct.jsonl")
         p.add_argument("--output_dir", type=str, default="out-qwen2-0p5b-lora")
         p.add_argument("--cutoff_len", type=int, default=256)  # 4GB VRAM 안전 구간
         p.add_argument("--max_steps", type=int, default=100)   # 100 step ~ 10~30분 예상(환경차 큼)
         p.add_argument("--lr", type=float, default=2e-4)
         p.add_argument("--per_device_train_batch_size", type=int, default=1)
         p.add_argument("--gradient_accumulation_steps", type=int, default=8)  # 유효 배치 8
     
         # [추가] W&B on/off 및 프로젝트/엔터티 설정
         p.add_argument("--use_wandb", action="store_true", help="Enable Weights & Biases logging")
         p.add_argument("--wandb_project", type=str, default="peft_train_test")
         p.add_argument("--wandb_entity", type=str, default=None, help="W&B team/user name(optional)")
     
         return p.parse_args()
     
     def main():
         args = parse_args()
         os.environ["TOKENIZERS_PARALLELISM"] = "false"
     
         # [추가] W&B 초기화 (옵션)
         if args.use_wandb:
             run_name = f"{args.model.split('/')[-1]}-lora-{int(time.time())}"
             wandb.init(
                 project=args.wandb_project,
                 entity=args.wandb_entity,   # 팀/조직 사용 시 지정, 개인 계정이면 None 가능
                 name=run_name,
                 config={
                     "model": args.model,
                     "data_path": args.data_path,
                     "cutoff_len": args.cutoff_len,
                     "max_steps": args.max_steps,
                     "lr": args.lr,
                     "per_device_train_batch_size": args.per_device_train_batch_size,
                     "gradient_accumulation_steps": args.gradient_accumulation_steps,
                     "method": "QLoRA + SFTTrainer",
                 },
                 settings=wandb.Settings(_disable_stats=False)  # GPU/시스템 메트릭 수집
             )
     
         # 4bit 양자화(QLoRA) 설정
         quant_config = BitsAndBytesConfig(
             load_in_4bit=True,
             bnb_4bit_use_double_quant=True,
             bnb_4bit_quant_type="nf4",
             bnb_4bit_compute_dtype=torch.float16  # Pascal(6.1)에서는 bf16 대신 fp16 권장
         )
     
         tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
     
         # 모델 로드 (자동 디바이스 매핑)
         model = AutoModelForCausalLM.from_pretrained(
             args.model,
             quantization_config=quant_config,
             torch_dtype=torch.float16,
             device_map="auto",
         )
     
         # 데이터셋 로드(JSONL: messages list)
         ds = load_dataset("json", data_files=args.data_path, split="train")
     
         # chat 템플릿으로 프리렌더링(text 생성)
         def to_text(example):
             messages = example["messages"]
             text = tokenizer.apply_chat_template(
                 messages,
                 tokenize=False,
                 add_generation_prompt=False
             )
             return {"text": text}
     
         ds = ds.map(to_text, remove_columns=ds.column_names)
     
         # LoRA 설정
         peft_cfg = LoraConfig(
             r=8, lora_alpha=16, lora_dropout=0.05,
             bias="none", task_type="CAUSAL_LM",
             target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
         )
     
         # 학습 세팅
         train_args = TrainingArguments(
             output_dir=args.output_dir,
             per_device_train_batch_size=args.per_device_train_batch_size,
             gradient_accumulation_steps=args.gradient_accumulation_steps,
             learning_rate=args.lr,
             logging_steps=10,            # <- W&B 그래프 업데이트 주기
             bf16=False,
             fp16=True,
             max_steps=args.max_steps,
             save_steps=args.max_steps,
             save_total_limit=1,
             optim="paged_adamw_8bit",   # bitsandbytes 옵티마이저
             lr_scheduler_type="cosine",
             warmup_ratio=0.03,
              # [변경] W&B 사용 여부에 따라 report_to 설정
             report_to=["wandb"] if args.use_wandb else "none",
             logging_dir="./logs",        # 로컬 로그도 남겨두면 TB로도 볼 수 있음
         )
     
         # SFTTrainer 사용 (언어모델링 라벨 자동 생성)
         trainer = SFTTrainer(
             model=model,
             tokenizer=tokenizer,
             peft_config=peft_cfg,
             train_dataset=ds,
             dataset_text_field="text",
             max_seq_length=args.cutoff_len,
             packing=False,  # 예시 데이터가 짧아도 안전하게 off
             args=train_args,
         )
     
         trainer.train()
         trainer.save_model(args.output_dir)  # 어댑터+메타 저장
         tokenizer.save_pretrained(args.output_dir)
     
         # [추가] W&B 종료
         if args.use_wandb:
             wandb.finish()
     if __name__ == "__main__":
         main()
          ```
     </details>
5. W&B Project 생성
   - W&B 사이트             return {"text": text}
     
         ds = ds.map(to_text, remove_columns=ds.column_names)
     
         # LoRA 설정
         peft_cfg = LoraConfig(
             r=8, lora_alpha=16, lora_dropout=0.05,
             bias="none", task_type="CAUSAL_LM",
             target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
         )
     
         # 학습 세팅
         train_args = TrainingArguments(
             output_dir=args.output_dir,
             per_device_train_batch_size=args.per_device_train_batch_size,
             gradient_accumulation_steps=args.gradient_accumulation_steps,
             learning_rate=args.lr,
             logging_steps=10,            # <- W&B 그래프 업데이트 주기
             bf16=False,
             fp16=True,
             max_steps=args.max_steps,
             save_steps=args.max_steps,
             save_total_limit=1,
             optim="paged_adamw_8bit",   # bitsandbytes 옵티마이저
             lr_scheduler_type="cosine",
             warmup_ratio=0.03,
              # [변경] W&B 사용 여부에 따라 report_to 설정
             report_to=["wandb"] if args.use_wandb else "none",
             logging_dir="./logs",        # 로컬 로그도 남겨두면 TB로도 볼 수 있음
         )
     
         # SFTTrainer 사용 (언어모델링 라벨 자동 생성)
         trainer = SFTTrainer(
             model=model,
             tokenizer=tokenizer,
             peft_config=peft_cfg,
             train_dataset=ds,
             dataset_text_field="text",
             max_seq_length=args.cutoff_len,
             packing=False,  # 예시 데이터가 짧아도 안전하게 off
             args=train_args,
         )
     
         trainer.train()
         trainer.save_model(args.output_dir)  # 어댑터+메타 저장
         tokenizer.save_pretrained(args.output_dir)
     
         # [추가] W&B 종료
         if args.use_wandb:
             wandb.finish()
     if __name__ == "__main__":
         main()
          ```
     </details>
     
5. W&B Project 생성
   - W&B 웹 사이트에서 Projects 탭에서 원하는 이름의 프로젝트를 생성.(아주 간단함)
  
6. W&B 로그인
   - 파이썬 가상환경을 실행하고 파이썬 코드를 실행하기 전에 W&B 로그인을 한다.
     - pip install wandb
     - wandb login
     - 실행 로그
     - 
     



     
