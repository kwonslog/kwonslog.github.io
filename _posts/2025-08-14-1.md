---
published: false
layout: single
title: "LLM 학습 과정 로그 분석"
excerpt: "wandb 를 이용한 학습 과정 로그 분석"
date: 2025-08-14
last_modified_at: 2025-08-14
categories: [AI]
tags: [LLM, wandb]
toc: true
---

# 체크리스트
 - [ ] 수집 된 지표들에 대한 설명중에 이해 안되는 부분 확인하기.
 - [ ] 파이썬 최신 코드로 게시물 수정하기.
 - [ ] 파이썬 최신 코드 실행 후 학습 과정의 콘솔 로그로 게시물 수정하기.



       

## 시작
- [로컬 PC에서 LLM 학습 시도]({% post_url 2025-08-13-1 %}) 게시물에서 개선방향에 대해 언급했다.
- 그 중에서 파인 튜닝 할때 발생하는 로그를 W&B 와 연동하여 시각화 하는 부분에 대한 과정을 정리해 본다.

## 시각화 과정
1. 시각화 도구 선정(Weights & Biases(W&B))
   - 많이 알려진 도구 일 것.
   - 분석 정보를 직관적으로 알기 쉬울 것.
   - 로그 전송을 위한 연동 과정이 쉬울 것.
  
2. W&B 회원 가입(https://wandb.ai/)
   - 회원 가입 할때 W&B Models, W&B Weave 를 선택 할 수 있는데 전자를 선택하여 진행함.
   - 가입 완료 후 Home 에 API Key 를 확인 할 수 있다.
   - 체험판 형태로 가입이 되며 기간이 끝나면 무료 플랜으로 이용 가능.
  
3. 기존 코드 수정
   - train_peft.py 파일에 W&B 연동을 위한 정보를 추가 한 train_peft2_sftconfig.py 파일 작성
   - <details markdown="1">
     <summary>코드</summary>
     
     ```python
     import os, argparse, math, time
     import torch
     from datasets import load_dataset
     from transformers import (
         AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainerCallback
     )
     from peft import LoraConfig
     from trl import SFTTrainer, SFTConfig
     
     # [옵션] W&B
     import wandb
     
     def parse_args():
         p = argparse.ArgumentParser()
         p.add_argument("--model", type=str, default="Qwen/Qwen2-0.5B-Instruct")
         p.add_argument("--data_path", type=str, default="mini_instruct.jsonl")
         p.add_argument("--output_dir", type=str, default="out-qwen2-0p5b-lora")
         p.add_argument("--cutoff_len", type=int, default=256)
         p.add_argument("--max_steps", type=int, default=100)
         p.add_argument("--lr", type=float, default=2e-4)
         p.add_argument("--per_device_train_batch_size", type=int, default=1)
         p.add_argument("--gradient_accumulation_steps", type=int, default=8)
         p.add_argument("--weight_decay", type=float, default=0.0)
         p.add_argument("--lora_dropout", type=float, default=0.05)
         p.add_argument("--val_ratio", type=float, default=0.15)
         p.add_argument("--eval_steps", type=int, default=20)
         p.add_argument("--save_steps", type=int, default=20)
         p.add_argument("--seed", type=int, default=42)
     
         # W&B
         p.add_argument("--use_wandb", action="store_true")
         p.add_argument("--wandb_project", type=str, default="peft_train_test")
         p.add_argument("--wandb_entity", type=str, default=None)
         return p.parse_args()
     
     def main():
         args = parse_args()
         os.environ["TOKENIZERS_PARALLELISM"] = "false"
         torch.manual_seed(args.seed)
     
         # ------ W&B init (옵션) ------
         if args.use_wandb:
             run_name = f"{args.model.split('/')[-1]}-lora-{int(time.time())}"
             wandb.init(
                 project=args.wandb_project,
                 entity=args.wandb_entity,
                 name=run_name,
                 config={
                     "model": args.model,
                     "data_path": args.data_path,
                     "cutoff_len": args.cutoff_len,
                     "max_steps": args.max_steps,
                     "lr": args.lr,
                     "per_device_train_batch_size": args.per_device_train_batch_size,
                     "gradient_accumulation_steps": args.gradient_accumulation_steps,
                     "method": "QLoRA + SFTTrainer (SFTConfig)",
                 },
                 settings=wandb.Settings(_disable_stats=False)  # GPU/시스템 메트릭
             )
     
         # ------ 4bit 양자화(QLoRA) ------
         quant_config = BitsAndBytesConfig(
             load_in_4bit=True,
             bnb_4bit_use_double_quant=True,
             bnb_4bit_quant_type="nf4",
             bnb_4bit_compute_dtype=torch.float16
         )
     
         tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
     
         model = AutoModelForCausalLM.from_pretrained(
             args.model,
             quantization_config=quant_config,
             torch_dtype=torch.float16,
             device_map="auto",
         )
     
         # ------ 데이터 로드 & 검증 분리 ------
         raw = load_dataset("json", data_files=args.data_path)["train"]
         split = raw.train_test_split(test_size=args.val_ratio, seed=args.seed)
         train_ds, val_ds = split["train"], split["test"]
     
         # chat 템플릿 프리렌더링(text 필드 생성)
         def to_text(example):
             messages = example["messages"]
             text = tokenizer.apply_chat_template(
                 messages, tokenize=False, add_generation_prompt=False
             )
             return {"text": text}
     
         train_ds = train_ds.map(to_text, remove_columns=train_ds.column_names)
         val_ds   = val_ds.map(to_text,   remove_columns=val_ds.column_names)
     
         # ------ LoRA 설정 ------
         peft_cfg = LoraConfig(
             r=8, lora_alpha=16, lora_dropout=args.lora_dropout,
             bias="none", task_type="CAUSAL_LM",
             target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
         )
     
         # ------ SFTConfig(TrainingArguments 대체) ------
         sft_cfg = SFTConfig(
             output_dir=args.output_dir,
             per_device_train_batch_size=args.per_device_train_batch_size,
             gradient_accumulation_steps=args.gradient_accumulation_steps,
             learning_rate=args.lr,
             weight_decay=args.weight_decay,
             logging_steps=10,
             bf16=False,
             fp16=True,
             max_steps=args.max_steps,
             save_steps=args.save_steps,
             save_total_limit=1,
             evaluation_strategy="steps",
             eval_steps=args.eval_steps,
             load_best_model_at_end=True,
             metric_for_best_model="eval_loss",
             greater_is_better=False,
             optim="paged_adamw_8bit",
             lr_scheduler_type="cosine",
             warmup_ratio=0.03,
             max_grad_norm=1.0,
             report_to=["wandb"] if args.use_wandb else "none",
             run_name=f"{args.model.split('/')[-1]}-lora" if args.use_wandb else None,
             seed=args.seed,
     
             # SFT 고유 옵션
             dataset_text_field="text",
             max_seq_length=args.cutoff_len,
             packing=False,
             gradient_checkpointing=True,        # VRAM 절약(속도 약간 저하)
         )
     
         # ------ Trainer ------
         trainer = SFTTrainer(
             model=model,
             tokenizer=tokenizer,
             peft_config=peft_cfg,
             train_dataset=train_ds,
             eval_dataset=val_ds,
             args=sft_cfg,
         )
     
         # ------ 평가 시 perplexity 계산/로깅 콜백 ------
         # (eval_loss가 산출된 후 exp(eval_loss) = perplexity를 추가로 기록)
         class PerplexityCallback(TrainerCallback):
             def on_evaluate(self, args, state, control, **kwargs):
                 metrics = kwargs.get("metrics", {})
                 if "eval_loss" in metrics and metrics["eval_loss"] is not None:
                     try:
                         ppl = math.exp(metrics["eval_loss"])
                     except OverflowError:
                         ppl = float("inf")
                     # 여기서 trainer.log(...) 호출 ❌
                     # metrics에만 추가하고 control 반환 ✅
                     metrics["perplexity"] = ppl
                 return control
     
         trainer.add_callback(PerplexityCallback())
     
         # ------ 학습 & 저장 ------
         trainer.train()
         trainer.save_model(args.output_dir)
         tokenizer.save_pretrained(args.output_dir)
     
         if args.use_wandb:
             wandb.finish()
     
     if __name__ == "__main__":
         main()
          
     ```
     
     </details>
     
5. W&B Project 생성
   - W&B 웹 사이트에서 Projects 탭에서 원하는 이름의 프로젝트를 생성.(아주 간단함)
  
6. W&B 로그인
   - 파이썬 가상환경을 실행하고 파이썬 코드를 실행하기 전에 W&B 로그인을 한다.
     - pip install wandb
     - wandb login
     - APi key 입력하면 끝.
     - 실행 로그
       <details>
         <summary>보기</summary>
         <pre>
         wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
         wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models
         wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:
         wandb: No netrc file found, creating one.
         wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\kwon\_netrc
         wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin           
         </pre>
       </details>
7. 코드 실행
   - python train_peft2_sftconfig3.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --eval_steps 20 --save_steps 20 --use_wandb --wandb_project local-pc-test
   - 마지막 --wandb_project 값은 W&B에서 생성한 프로젝트 이름을 사용했다.
   - 실행 로그
     <details>
       <summary>보기</summary>
       <pre>
       (.venv) C:\peft_test>python train_peft2_sftconfig3.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --eval_steps 20 --save_steps 20 --use_wandb --wandb_project local-pc-test
       wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
       wandb: Tracking run with wandb version 0.21.1
       wandb: Run data is saved locally in C:\peft_test\wandb\run-20250818_104047-14muyjlm
       wandb: Run `wandb offline` to turn off syncing.
       wandb: Syncing run Qwen2-0.5B-Instruct-lora-1755481247
       wandb:  View project at https://wandb.ai/kwonslog-study/local-pc-test
       wandb:  View run at https://wandb.ai/kwonslog-study/local-pc-test/runs/14muyjlm
       C:\peft_test\.venv\lib\site-packages\transformers\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
         warnings.warn(
       Map: 100%|████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 71.80 examples/s]
       C:\peft_test\.venv\lib\site-packages\accelerate\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
         self.scaler = torch.cuda.amp.GradScaler(**kwargs)
       max_steps is given, it will override any value given in num_train_epochs
         0%|                                                                                          | 0/100 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
       C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 2.7509, 'grad_norm': 1.7712396383285522, 'learning_rate': 0.0001981178176898239, 'epoch': 4.21}
       {'loss': 1.413, 'grad_norm': 2.205397129058838, 'learning_rate': 0.00018687117365181512, 'epoch': 8.42}
        20%|████████████████▏                                                                | 20/100 [01:31<06:21,  4.77s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
       {'eval_loss': 2.680995464324951, 'eval_runtime': 0.4523, 'eval_samples_per_second': 8.843, 'eval_steps_per_second': 2.211, 'epoch': 8.42}
        20%|████████████████▏                                                                | 20/100 [01:32<06:21,  4.77s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.6029, 'grad_norm': 1.7841202020645142, 'learning_rate': 0.00016659152250116812, 'epoch': 12.63}
       {'loss': 0.1577, 'grad_norm': 1.1527832746505737, 'learning_rate': 0.00013938757562492873, 'epoch': 16.84}
       {'eval_loss': 3.554626226425171, 'eval_runtime': 0.4171, 'eval_samples_per_second': 9.59, 'eval_steps_per_second': 2.397, 'epoch': 16.84}
        40%|████████████████████████████████▍                                                | 40/100 [03:10<04:50,  4.85s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0722, 'grad_norm': 0.8666030764579773, 'learning_rate': 0.00010808804403614043, 'epoch': 21.05}
       {'loss': 0.0632, 'grad_norm': 0.865715503692627, 'learning_rate': 7.594750436337467e-05, 'epoch': 25.26}
       {'eval_loss': 3.5855770111083984, 'eval_runtime': 0.4205, 'eval_samples_per_second': 9.512, 'eval_steps_per_second': 2.378, 'epoch': 25.26}
        60%|████████████████████████████████████████████████▌                                | 60/100 [04:49<03:15,  4.89s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0588, 'grad_norm': 0.6691780686378479, 'learning_rate': 4.630798263510162e-05, 'epoch': 29.47}
       {'loss': 0.0568, 'grad_norm': 0.6350418925285339, 'learning_rate': 2.2251444932035094e-05, 'epoch': 33.68}
       {'eval_loss': 3.633697271347046, 'eval_runtime': 0.4166, 'eval_samples_per_second': 9.602, 'eval_steps_per_second': 2.401, 'epoch': 33.68}
        80%|████████████████████████████████████████████████████████████████▊                | 80/100 [06:24<01:33,  4.67s/it]C:\peft_test\.venv\lib\site-packages\torch\_dynamo\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
         return fn(*args, **kwargs)
       {'loss': 0.0562, 'grad_norm': 0.6472147703170776, 'learning_rate': 6.2793294993656494e-06, 'epoch': 37.89}
       {'loss': 0.0554, 'grad_norm': 0.6560364365577698, 'learning_rate': 5.2443095448506674e-08, 'epoch': 42.11}
       {'eval_loss': 3.6372601985931396, 'eval_runtime': 0.4229, 'eval_samples_per_second': 9.458, 'eval_steps_per_second': 2.364, 'epoch': 42.11}
       {'train_runtime': 492.1056, 'train_samples_per_second': 1.626, 'train_steps_per_second': 0.203, 'train_loss': 0.5287103277444839, 'epoch': 42.11}
       100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [08:12<00:00,  4.92s/it]
       wandb:
       wandb:
       wandb: Run history:
       wandb:               eval/loss ▁▇███
       wandb:            eval/runtime █▁▂▁▂
       wandb: eval/samples_per_second ▁█▇█▇
       wandb:   eval/steps_per_second ▁█▇█▇
       wandb:             train/epoch ▁▂▂▃▃▃▄▅▅▆▆▆▇███
       wandb:       train/global_step ▁▂▂▃▃▃▄▅▅▆▆▆▇███
       wandb:         train/grad_norm ▆█▆▃▂▂▁▁▁▁
       wandb:     train/learning_rate ██▇▆▅▄▃▂▁▁
       wandb:              train/loss █▅▂▁▁▁▁▁▁▁
       wandb:
       wandb: Run summary:
       wandb:                eval/loss 3.63726
       wandb:             eval/runtime 0.4229
       wandb:  eval/samples_per_second 9.458
       wandb:    eval/steps_per_second 2.364
       wandb:               total_flos 99576665223168.0
       wandb:              train/epoch 42.10526
       wandb:        train/global_step 100
       wandb:          train/grad_norm 0.65604
       wandb:      train/learning_rate 0.0
       wandb:               train/loss 0.0554
       wandb:               train_loss 0.52871
       wandb:            train_runtime 492.1056
       wandb: train_samples_per_second 1.626
       wandb:   train_steps_per_second 0.203
       wandb:
       wandb:  View run Qwen2-0.5B-Instruct-lora-1755481247 at: https://wandb.ai/kwonslog-study/local-pc-test/runs/14muyjlm
       wandb:  View project at: https://wandb.ai/kwonslog-study/local-pc-test
       wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
       wandb: Find logs at: .\wandb\run-20250818_104047-14muyjlm\logs
       </pre>
     </details>
     
8. W&B 로그 확인
   - View project at: https://wandb.ai/kwonslog-study/local-pc-test
   - 위와 같이 확인 가능한 주소가 출력되며 해당 주소를 통해 시각화 된 데이터를 볼 수 있다.
   - 예시
     <img width="1788" height="821" alt="image" src="https://github.com/user-attachments/assets/0f0ceb55-3faa-48f7-93de-4b5f9e978b0d" />

## 시각화 데이터 분석
- 먼저 각 지표들의 개념에 대해서 알아보자.
- W&B 의 train 대시보드에는 아래 지표들에 대한 그래프가 보인다.
   - loss
   - learning_rate
   - grad_norm
   - global_step
   - epoch
 
### loss (손실값)
- 모델이 정답과 얼마나 다른 예측을 했는지 수치로 나타낸 값.
- 0에 가까울수록 모델 예측이 정답에 가깝다는 뜻.
- 학습에서는 손실 → 역전파 → 가중치 수정의 핵심 루프를 돕는 지표.

| 값 변화               | 해석 예시                                  |
| ------------------ | -------------------------------------- |
| 크다 (예: 5.0 이상) | 예측이 정답과 많이 다름 → 학습 초기거나, 학습이 잘 안 되는 상황 |
| 작다 (예: 0.5 이하) | 예측이 정답에 가까움 → 학습이 진행됨, 성능 좋아짐          |
| 줄다가 다시 커짐      | 과적합 가능성 → 학습 데이터엔 잘 맞지만 일반화가 안 되고 있음   |

  
### learning_rate (학습률)
- 한 번의 가중치 업데이트에서 얼마나 크게 움직일지를 정하는 값.
- 너무 크면 → 발산(학습 불안정), 너무 작으면 → 학습 속도가 매우 느려짐.
- 보통 스케줄러를 사용해 학습률을 점진적으로 줄이거나 조절.

| 값 변화                     | 해석 예시                          |
| ------------------------ | ------------------------------ |
| 크다 (예: 1e-2)         | 빠르게 학습하지만 불안정 → loss가 요동칠 수 있음 |
| 적당 (예: 1e-3 \~ 1e-4) | 안정적으로 학습됨 (많이 쓰는 범위)           |
| 아주 작다 (예: 1e-6 이하)   | 거의 학습이 안 됨, loss가 잘 안 줄어듦      |


### grad_norm (그래디언트 크기)
- 한 스텝에서 계산된 가중치 변화량(gradient)의 크기를 하나로 합친 값.
- 학습 안정성의 지표. 너무 크면 폭발, 너무 작으면 학습 멈춤.
- max_grad_norm으로 클리핑(제한) 가능.

| 값 변화                   | 해석 예시                                |
| ---------------------- | ------------------------------------ |
| 크다 (예: 10 이상)      | Gradient 폭발 가능성 → 학습 불안정, loss 급등 가능 |
| 적당 (예: 1\~5)       | 정상 범위, 안정적인 업데이트                     |
| 아주 작다 (예: 0.01 이하) | 학습 정체, 가중치가 거의 안 변함                  |

### global_step (전체 스텝 수)
- 지금까지 훈련 배치를 처리한 횟수.
- 학습 진행 정도를 나타내는 단순 카운터.
- 예: batch size=1, 데이터 100개 → 100스텝이 한 에폭.

| 값 변화        | 해석 예시                      |
| ----------- | -------------------------- |
| 작다 (초반) | 학습 막 시작, loss가 크게 변할 수 있음  |
| 크다 (후반) | 학습 후반부, loss가 안정되거나 과적합 가능 |

### epoch (에폭)
- 학습 데이터 전체를 한 번 다 본 주기.
- 1에폭 = 데이터 전부 학습 한 번 완료.
- 여러 에폭 반복하면서 점점 성능 향상.

| 값 변화              | 해석 예시                           |
| ----------------- | ------------------------------- |
| 작다 (1\~2)     | 학습 초기, 성능 향상 폭이 큼               |
| 중간 (3\~10)    | 안정화 단계, loss 감소폭이 줄어듦           |
| 너무 크다 (20 이상) | 과적합 위험 → eval 성능이 떨어지기 시작할 수 있음 |

### 종합 해석 예시
- 정상 학습 패턴
  - train loss: 점점 하락
  - learning_rate: 스케줄에 따라 서서히 감소
  - grad_norm: 안정 범위(1\~5) 유지
  - global_step / epoch: 꾸준히 증가
    
- 문제 패턴 예시
  - loss가 줄지 않음 → learning_rate 너무 낮음 or 데이터 품질 문제
  - grad_norm이 계속 높음 → learning_rate 과다 or 모델 불안정
  - train loss는 낮지만 eval loss 상승 → 과적합

### W&B 의 지표 분석 
- 연동한 W&B의 지표(그래프)를 보고 학습 상황이 어떤지 분석해 본다.
  
1. 과적합
<img width="1284" height="433" alt="image" src="https://github.com/user-attachments/assets/008f0b12-ee9d-4776-803a-ced682f3bd7b" />

- train/loss
  - 학습 스텝이 진행될수록 빠르게 감소 → 모델이 학습 데이터에 점점 잘 맞추고 있음.
  - 40스텝 이후 거의 0에 가까운 값 → 학습 데이터는 완벽하게 외워가는 상태.
- eval/loss
  - 초기엔 낮았다가, 스텝이 진행될수록 오히려 상승 → 새로운 데이터(검증셋)에서는 성능이 점점 나빠짐.
  - 20스텝 이후부터는 지속적으로 증가 → 학습 데이터에만 특화되고 일반화 능력 저하.

| 지표 변화                   | 의미                |
| ----------------------- | ----------------- |
| **train loss ↓↓**       | 학습 데이터 예측 정확도 급상승 |
| **eval loss ↑↑**        | 검증 데이터 예측 성능 하락   |
| train / eval 곡선의 방향이 반대 | **전형적인 과적합** 패턴   |

     
