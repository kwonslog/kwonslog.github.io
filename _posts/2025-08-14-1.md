---
published: false
layout: single
title: "LLM 학습 과정 로그 분석"
excerpt: "wandb 를 이용한 학습 과정 로그 분석"
date: 2025-08-14
last_modified_at: 2025-08-14
categories: [AI]
tags: [LLM, wandb]
toc: true
---

## 시작
- [로컬 PC에서 LLM 학습 시도]({% post_url 2025-08-13-1 %}) 게시물에서 개선방향에 대해 언급했다.
- 그 중에서 파인 튜닝 할때 발생하는 로그를 W&B 와 연동하여 시각화 하는 부분에 대한 과정을 정리해 본다.

## 시각화 과정
1. 시각화 도구 선정(Weights & Biases(W&B))
   - 많이 알려진 도구 일 것.
   - 분석 정보를 직관적으로 알기 쉬울 것.
   - 로그 전송을 위한 연동 과정이 쉬울 것.
  
2. W&B 회원 가입(https://wandb.ai/)
   - 회원 가입 할때 W&B Models, W&B Weave 를 선택 할 수 있는데 전자를 선택하여 진행함.
   - 가입 완료 후 Home 에 API Key 를 확인 할 수 있다.
   - 체험판 형태로 가입이 되며 기간이 끝나면 무료 플랜으로 이용 가능.
  
4. 기존 코드 수정
   - train_peft.py 파일에 W&B 연동을 위한 정보를 추가 한다.
   - <details markdown="1">
     <summary>코드</summary>
     
     ```python
     # 추가 학습한 어댑터를 생성 + 생성과정 시각화(W&B) 연동
     
     import os, argparse
     import torch
     from datasets import load_dataset
     from transformers import (
         AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
         TrainingArguments
     )
     from peft import LoraConfig
     from trl import SFTTrainer
     
     # [추가] W&B
     import time
     import wandb
     
     def parse_args():
         p = argparse.ArgumentParser()
         p.add_argument("--model", type=str, default="Qwen/Qwen2-0.5B-Instruct")
         p.add_argument("--data_path", type=str, default="mini_instruct.jsonl")
         p.add_argument("--output_dir", type=str, default="out-qwen2-0p5b-lora")
         p.add_argument("--cutoff_len", type=int, default=256)  # 4GB VRAM 안전 구간
         p.add_argument("--max_steps", type=int, default=100)   # 100 step ~ 10~30분 예상(환경차 큼)
         p.add_argument("--lr", type=float, default=2e-4)
         p.add_argument("--per_device_train_batch_size", type=int, default=1)
         p.add_argument("--gradient_accumulation_steps", type=int, default=8)  # 유효 배치 8
     
         # [추가] W&B on/off 및 프로젝트/엔터티 설정
         p.add_argument("--use_wandb", action="store_true", help="Enable Weights & Biases logging")
         p.add_argument("--wandb_project", type=str, default="peft_train_test")
         p.add_argument("--wandb_entity", type=str, default=None, help="W&B team/user name(optional)")
     
         return p.parse_args()
     
     def main():
         args = parse_args()
         os.environ["TOKENIZERS_PARALLELISM"] = "false"
     
         # [추가] W&B 초기화 (옵션)
         if args.use_wandb:
             run_name = f"{args.model.split('/')[-1]}-lora-{int(time.time())}"
             wandb.init(
                 project=args.wandb_project,
                 entity=args.wandb_entity,   # 팀/조직 사용 시 지정, 개인 계정이면 None 가능
                 name=run_name,
                 config={
                     "model": args.model,
                     "data_path": args.data_path,
                     "cutoff_len": args.cutoff_len,
                     "max_steps": args.max_steps,
                     "lr": args.lr,
                     "per_device_train_batch_size": args.per_device_train_batch_size,
                     "gradient_accumulation_steps": args.gradient_accumulation_steps,
                     "method": "QLoRA + SFTTrainer",
                 },
                 settings=wandb.Settings(_disable_stats=False)  # GPU/시스템 메트릭 수집
             )
     
         # 4bit 양자화(QLoRA) 설정
         quant_config = BitsAndBytesConfig(
             load_in_4bit=True,
             bnb_4bit_use_double_quant=True,
             bnb_4bit_quant_type="nf4",
             bnb_4bit_compute_dtype=torch.float16  # Pascal(6.1)에서는 bf16 대신 fp16 권장
         )
     
         tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=True)
         if tokenizer.pad_token is None:
             tokenizer.pad_token = tokenizer.eos_token
     
         # 모델 로드 (자동 디바이스 매핑)
         model = AutoModelForCausalLM.from_pretrained(
             args.model,
             quantization_config=quant_config,
             torch_dtype=torch.float16,
             device_map="auto",
         )
     
         # 데이터셋 로드(JSONL: messages list)
         ds = load_dataset("json", data_files=args.data_path, split="train")
     
         # chat 템플릿으로 프리렌더링(text 생성)
         def to_text(example):
             messages = example["messages"]
             text = tokenizer.apply_chat_template(
                 messages,
                 tokenize=False,
                 add_generation_prompt=False
             )
             return {"text": text}
     
         ds = ds.map(to_text, remove_columns=ds.column_names)
     
         # LoRA 설정
         peft_cfg = LoraConfig(
             r=8, lora_alpha=16, lora_dropout=0.05,
             bias="none", task_type="CAUSAL_LM",
             target_modules=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
         )
     
         # 학습 세팅
         train_args = TrainingArguments(
             output_dir=args.output_dir,
             per_device_train_batch_size=args.per_device_train_batch_size,
             gradient_accumulation_steps=args.gradient_accumulation_steps,
             learning_rate=args.lr,
             logging_steps=10,            # <- W&B 그래프 업데이트 주기
             bf16=False,
             fp16=True,
             max_steps=args.max_steps,
             save_steps=args.max_steps,
             save_total_limit=1,
             optim="paged_adamw_8bit",   # bitsandbytes 옵티마이저
             lr_scheduler_type="cosine",
             warmup_ratio=0.03,
              # [변경] W&B 사용 여부에 따라 report_to 설정
             report_to=["wandb"] if args.use_wandb else "none",
             logging_dir="./logs",        # 로컬 로그도 남겨두면 TB로도 볼 수 있음
         )
     
         # SFTTrainer 사용 (언어모델링 라벨 자동 생성)
         trainer = SFTTrainer(
             model=model,
             tokenizer=tokenizer,
             peft_config=peft_cfg,
             train_dataset=ds,
             dataset_text_field="text",
             max_seq_length=args.cutoff_len,
             packing=False,  # 예시 데이터가 짧아도 안전하게 off
             args=train_args,
         )
     
         trainer.train()
         trainer.save_model(args.output_dir)  # 어댑터+메타 저장
         tokenizer.save_pretrained(args.output_dir)
     
         # [추가] W&B 종료
         if args.use_wandb:
             wandb.finish()
     if __name__ == "__main__":
         main()     
     ```
     
     </details>
     
5. W&B Project 생성
   - W&B 웹 사이트에서 Projects 탭에서 원하는 이름의 프로젝트를 생성.(아주 간단함)
  
6. W&B 로그인
   - 파이썬 가상환경을 실행하고 파이썬 코드를 실행하기 전에 W&B 로그인을 한다.
     - pip install wandb
     - wandb login
     - APi key 입력하면 끝.
     - 실행 로그
       <details>
         <summary>보기</summary>
         <pre>
         wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
         wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models
         wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:
         wandb: No netrc file found, creating one.
         wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\kwon\_netrc
         wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin           
         </pre>
       </details>
7. 코드 실행
   - python train_peft2.py --max_steps 100 --cutoff_len 256 --per_device_train_batch_size 1 --gradient_accumulation_steps 8 --use_wandb --wandb_project local-pc-test
   - 마지막 --wandb_project 값은 W&B에서 생성한 프로젝트 이름을 사용했다.
   - 실행 로그
     <details>
       <summary>보기</summary>
       <pre>
       wandb: Currently logged in as: kwonslog (kwonslog-study) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
       wandb: Tracking run with wandb version 0.21.1
       wandb: Run data is saved locally in C:\peft_test\wandb\run-20250814_102356-qhkto1cb
       wandb: Run `wandb offline` to turn off syncing.
       wandb: Syncing run Qwen2-0.5B-Instruct-lora-1755134635
       wandb:  View project at https://wandb.ai/kwonslog-study/local-pc-test
       wandb:  View run at https://wandb.ai/kwonslog-study/local-pc-test/runs/qhkto1cb
       Map: 100%|█████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 178.52 examples/s]
       C:\peft_test\.venv\lib\site-packages\huggingface_hub\utils\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.
       
       Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
         warnings.warn(message, FutureWarning)
       C:\peft_test\.venv\lib\site-packages\trl\trainer\sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
         warnings.warn(
       C:\peft_test\.venv\lib\site-packages\trl\trainer\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
         warnings.warn(
       Map: 100%|█████████████████████████████████████████████████████████████| 23/23 [00:00<00:00, 572.65 examples/s]
       C:\peft_test\.venv\lib\site-packages\accelerate\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
         self.scaler = torch.cuda.amp.GradScaler(**kwargs)
       max_steps is given, it will override any value given in num_train_epochs
       wandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
       {'loss': 2.8012, 'grad_norm': 1.8380658626556396, 'learning_rate': 0.0001981178176898239, 'epoch': 3.48}
       {'loss': 1.5447, 'grad_norm': 1.8809651136398315, 'learning_rate': 0.00018687117365181512, 'epoch': 6.96}
       {'loss': 0.7899, 'grad_norm': 2.3696653842926025, 'learning_rate': 0.00016659152250116812, 'epoch': 10.43}
       {'loss': 0.2815, 'grad_norm': 1.485860824584961, 'learning_rate': 0.00013938757562492873, 'epoch': 13.91}
       {'loss': 0.0948, 'grad_norm': 0.8242879509925842, 'learning_rate': 0.00010808804403614043, 'epoch': 17.39}
       {'loss': 0.071, 'grad_norm': 0.6168610453605652, 'learning_rate': 7.594750436337467e-05, 'epoch': 20.87}
       {'loss': 0.0637, 'grad_norm': 0.7342116236686707, 'learning_rate': 4.630798263510162e-05, 'epoch': 24.35}
       {'loss': 0.0622, 'grad_norm': 0.7220414280891418, 'learning_rate': 2.2251444932035094e-05, 'epoch': 27.83}
       {'loss': 0.06, 'grad_norm': 0.8508087992668152, 'learning_rate': 6.2793294993656494e-06, 'epoch': 31.3}
       {'loss': 0.0599, 'grad_norm': 0.7569730281829834, 'learning_rate': 5.2443095448506674e-08, 'epoch': 34.78}
       {'train_runtime': 345.2677, 'train_samples_per_second': 2.317, 'train_steps_per_second': 0.29, 'train_loss': 0.5828885662555695, 'epoch': 34.78}
       100%|████████████████████████████████████████████████████████████████████████| 100/100 [05:45<00:00,  3.45s/it]
       wandb:
       wandb:
       wandb: Run history:
       wandb:         train/epoch ▁▂▃▃▄▅▆▆▇██
       wandb:   train/global_step ▁▂▃▃▄▅▆▆▇██
       wandb:     train/grad_norm ▆▆█▄▂▁▁▁▂▂
       wandb: train/learning_rate ██▇▆▅▄▃▂▁▁
       wandb:          train/loss █▅▃▂▁▁▁▁▁▁
       wandb:
       wandb: Run summary:
       wandb:               total_flos 100896151683840.0
       wandb:              train/epoch 34.78261
       wandb:        train/global_step 100
       wandb:          train/grad_norm 0.75697
       wandb:      train/learning_rate 0.0
       wandb:               train/loss 0.0599
       wandb:               train_loss 0.58289
       wandb:            train_runtime 345.2677
       wandb: train_samples_per_second 2.317
       wandb:   train_steps_per_second 0.29
       wandb:
       wandb:  View run Qwen2-0.5B-Instruct-lora-1755134635 at: https://wandb.ai/kwonslog-study/local-pc-test/runs/qhkto1cb
       wandb:  View project at: https://wandb.ai/kwonslog-study/local-pc-test
       wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
       wandb: Find logs at: .\wandb\run-20250814_102356-qhkto1cb\logs         
       </pre>
     </details>
8. W&B 로그 확인
   - View project at: https://wandb.ai/kwonslog-study/local-pc-test
   - 위와 같이 확인 가능한 주소가 출력되며 해당 주소를 통해 시각화 된 데이터를 볼 수 있다.
   - 예시
     <img width="1788" height="821" alt="image" src="https://github.com/user-attachments/assets/0f0ceb55-3faa-48f7-93de-4b5f9e978b0d" />

## 시각화 데이터 분석
- 먼저 각 지표들의 개념에 대해서 알아보자.
- W&B 의 train 대시보드에는 아래 지표들에 대한 그래프가 보인다.
   - loss
   - learning_rate
   - grad_norm
   - global_step
   - epoch
 
### loss (손실값)
- 모델이 정답과 얼마나 다른 예측을 했는지 수치로 나타낸 값.
- 0에 가까울수록 모델 예측이 정답에 가깝다는 뜻.
- 학습에서는 손실 → 역전파 → 가중치 수정의 핵심 루프를 돕는 지표.

| 값 변화               | 해석 예시                                  |
| ------------------ | -------------------------------------- |
| 크다 (예: 5.0 이상) | 예측이 정답과 많이 다름 → 학습 초기거나, 학습이 잘 안 되는 상황 |
| 작다 (예: 0.5 이하) | 예측이 정답에 가까움 → 학습이 진행됨, 성능 좋아짐          |
| 줄다가 다시 커짐      | 과적합 가능성 → 학습 데이터엔 잘 맞지만 일반화가 안 되고 있음   |
- :question:
  
### learning_rate (학습률)
- 한 번의 가중치 업데이트에서 얼마나 크게 움직일지를 정하는 값.
- 너무 크면 → 발산(학습 불안정), 너무 작으면 → 학습 속도가 매우 느려짐.
- 보통 스케줄러를 사용해 학습률을 점진적으로 줄이거나 조절.

| 값 변화                     | 해석 예시                          |
| ------------------------ | ------------------------------ |
| 크다 (예: 1e-2)         | 빠르게 학습하지만 불안정 → loss가 요동칠 수 있음 |
| 적당 (예: 1e-3 \~ 1e-4) | 안정적으로 학습됨 (많이 쓰는 범위)           |
| 아주 작다 (예: 1e-6 이하)   | 거의 학습이 안 됨, loss가 잘 안 줄어듦      |

- :question:

### grad_norm (그래디언트 크기)
- 한 스텝에서 계산된 가중치 변화량(gradient)의 크기를 하나로 합친 값.
- 학습 안정성의 지표. 너무 크면 폭발, 너무 작으면 학습 멈춤.
- max_grad_norm으로 클리핑(제한) 가능.

| 값 변화                   | 해석 예시                                |
| ---------------------- | ------------------------------------ |
| 크다 (예: 10 이상)      | Gradient 폭발 가능성 → 학습 불안정, loss 급등 가능 |
| 적당 (예: 1\~5)       | 정상 범위, 안정적인 업데이트                     |
| 아주 작다 (예: 0.01 이하) | 학습 정체, 가중치가 거의 안 변함                  |

### global_step (전체 스텝 수)
- 지금까지 훈련 배치를 처리한 횟수.
- 학습 진행 정도를 나타내는 단순 카운터.
- 예: batch size=1, 데이터 100개 → 100스텝이 한 에폭.

| 값 변화        | 해석 예시                      |
| ----------- | -------------------------- |
| 작다 (초반) | 학습 막 시작, loss가 크게 변할 수 있음  |
| 크다 (후반) | 학습 후반부, loss가 안정되거나 과적합 가능 |

### epoch (에폭)
- 학습 데이터 전체를 한 번 다 본 주기.
- 1에폭 = 데이터 전부 학습 한 번 완료.
- 여러 에폭 반복하면서 점점 성능 향상.

| 값 변화              | 해석 예시                           |
| ----------------- | ------------------------------- |
| 작다 (1\~2)     | 학습 초기, 성능 향상 폭이 큼               |
| 중간 (3\~10)    | 안정화 단계, loss 감소폭이 줄어듦           |
| 너무 크다 (20 이상) | 과적합 위험 → eval 성능이 떨어지기 시작할 수 있음 |

### 종합 해석 예시
- 정상 학습 패턴
  - train loss: 점점 하락
  - learning_rate: 스케줄에 따라 서서히 감소
  - grad_norm: 안정 범위(1\~5) 유지
  - global_step / epoch: 꾸준히 증가
    
- 문제 패턴 예시
  - loss가 줄지 않음 → learning_rate 너무 낮음 or 데이터 품질 문제
  - grad_norm이 계속 높음 → learning_rate 과다 or 모델 불안정
  - train loss는 낮지만 eval loss 상승 → 과적

     
